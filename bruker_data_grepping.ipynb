{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e79f5a-afdd-4b77-a0cc-e9a072fb697f",
   "metadata": {},
   "source": [
    "# Bruker Data Grepping and Alignment Notebook\n",
    "### Jeremy Delahanty June 2021\n",
    "\n",
    "Intended to grep different files/projects/datasets from user input and retain them for use in analysis/display later. The lack of unified filenaming structures between projects will break the code... A convetion of XXX### for animal names has been established for Austin's projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "236756af-545f-4c82-bfc7-7f22a45d8a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "def109c7-4a96-41a8-b0c5-a66ab01b0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_basepath = \"Y:/\"\n",
    "# project_dict = {\"specialk\": [\"learned_helplessness\", \"chronic_mild_stress\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "62578e70-c52a-4a7e-9d62-b7c5d0b02268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found All Selected Teams\n",
      "Teams Returned:\n",
      "specialk \n"
     ]
    }
   ],
   "source": [
    "def grep_teams(team_selection=[], lab_basepath=\"Y:/\"):\n",
    "    \"\"\"\n",
    "    Grabs team list from server based on user's input.\n",
    "    \n",
    "    User can define which teams they want to use for their analyses and\n",
    "    the function will glob the paths for their selection.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of strings for teams of interest\n",
    "        Default is empty list\n",
    "    arg2: string\n",
    "        Basepath for server location on machine\n",
    "        Default is Y:/ for mapped Windows drive\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team path grabbed from server successfully\n",
    "    2. list\n",
    "        List of teams not found\n",
    "    \"\"\"\n",
    "\n",
    "    # Take basepath and glob all available files and directories\n",
    "    team_glob = Path(lab_basepath).glob(\"{}\".format(\"*\"))\n",
    "    \n",
    "    # Check if no team was specifically asked for, tell user we're gathering all teams\n",
    "    if team_selection == []:\n",
    "\n",
    "        print(\"Gathering all teams...\")\n",
    "\n",
    "        # List comprehension for returning all directories in Tye Lab server\n",
    "        team_list = [team for team in team_glob if team.is_dir()]\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # List comprehension for returning only directories user wants in the Tye Lab server \n",
    "        team_list = [team for team in team_glob if team.name in team_selection and team.is_dir()]\n",
    "    \n",
    "    # Create temporary list for checking if selected teams exist\n",
    "    tmp = []\n",
    "\n",
    "    # For the teams that were globbed successfully, append the team to the temp list\n",
    "    for globbed_team in team_list:\n",
    "        tmp.append(globbed_team.name)\n",
    "    \n",
    "    # Compare team selection with returned teams using sets, convert to list\n",
    "    missing_teams = list(set(team_selection) - set(tmp))\n",
    "    \n",
    "    # If the missing_teams list is empty, the program found all requested teams\n",
    "    if missing_teams == []:\n",
    "        print(\"Found All Selected Teams\")\n",
    "    \n",
    "    # Else, some teams weren't found. Tell the user which teams weren't found.\n",
    "    else:\n",
    "        print(\"Failed to find team(s):\", missing_teams)\n",
    "   \n",
    "    # Show user which teams were returned\n",
    "    print(\"Teams Returned:\")\n",
    "    for team in team_list:\n",
    "        print(\"{} \".format(team.name))\n",
    "    \n",
    "    # Return the list of projects gathered\n",
    "    return team_list, missing_teams\n",
    "\n",
    "team_list, missing_teams = grep_teams([\"specialk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d47d606e-6e66-4ffb-a644-1af2834b8e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned Directories: \n",
      "Y:\\specialk\\learned_helplessness\n"
     ]
    }
   ],
   "source": [
    "def choose_projects(team_list, project_selection={}):\n",
    "    \"\"\"\n",
    "    Generates project paths list based on user's selection.\n",
    "    \n",
    "    User can define which project they want to use for their analyses and\n",
    "    this function generates the paths for their selection.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of strings for teams of interest from grep_teams()\n",
    "    arg2: dict\n",
    "        Dictionary of values that will be used to create specific\n",
    "        paths for selected teams and their projects\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team/project Paths to grep in next steps\n",
    "    \"\"\"\n",
    "\n",
    "    # Make dictionary using the teams in team_list as keys\n",
    "    project_dict = {team: [] for team in team_list}\n",
    "\n",
    "    # For each time in the team_list, append the Path name's project's values\n",
    "    for team in team_list:\n",
    "        project_dict[team].append(project_selection[team.name])\n",
    "    \n",
    "    # Make empty project list\n",
    "    project_dir_list = []\n",
    "\n",
    "    for team in project_dict.keys():\n",
    "        for project in range(len(project_dict[team][0])):\n",
    "            project_dir_list.append(team / project_dict[team][0][project])\n",
    "    \n",
    "    print(\"Returned Directories: \")\n",
    "\n",
    "    for directory in project_dir_list:\n",
    "        print(directory)\n",
    "    \n",
    "    return project_dir_list\n",
    "    \n",
    "project_list = choose_projects(team_list, project_selection={\"specialk\": [\"learned_helplessness\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d6c423b7-92e8-45ba-9af8-4103534166d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing only E animals...\n",
      "learned_helplessness LHE011\n",
      "learned_helplessness LHE012\n",
      "learned_helplessness LHE013\n",
      "learned_helplessness LHE014\n",
      "learned_helplessness LHE015\n",
      "learned_helplessness LHE016\n"
     ]
    }
   ],
   "source": [
    "def choose_animals(project_list, animal_group=\"all\"):\n",
    "    \"\"\"\n",
    "    Generates animal paths list based on user's selection.\n",
    "    \n",
    "    User can define which cohort of animals they want to use \n",
    "    for their analyses. This function generates the paths for \n",
    "    their selection that meet specified conditions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of strings for projects of interest from choose_projects()\n",
    "    arg2: str\n",
    "        String of value for which animal paths to gather.\n",
    "        Default value is all.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team/project/animal Paths to grep in next steps\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create empty animal list for path generation\n",
    "    animal_list = []\n",
    "    \n",
    "    # If the animal group is left as default/specified as all, grab all animals\n",
    "    if animal_group == \"all\":\n",
    "        print(\"Grabbing all animals...\")\n",
    "        \n",
    "        # For each project directory in the project list\n",
    "        for project_dir in project_list:\n",
    "            \n",
    "            # For each animal globbed in the project directory\n",
    "            for animal in project_dir.glob(\"*\"):\n",
    "                \n",
    "                # Append the animal's path to the animal_list\n",
    "                print(project_dir.name, animal.name)\n",
    "                animal_list.append(animal)\n",
    "    \n",
    "    # Else, only select animals from the specified group\n",
    "    else:\n",
    "        print(\"Grabbing only {} animals...\".format(animal_group))\n",
    "\n",
    "        # Format the animal group with the user's input\n",
    "        animal_group = \"[A-Z]{2}\" + animal_group + \"\\d{3}\"\n",
    "        \n",
    "        # For each project_directory in project_list\n",
    "        for project_dir in project_list:\n",
    "\n",
    "            # For each animal globbed in project directory\n",
    "            for animal in project_dir.glob(\"*\"):\n",
    "                \n",
    "                # Use regex to grab only the requested animal\n",
    "                r = re.search(animal_group, string=animal.name)\n",
    "                \n",
    "                # If the search returns None, the animal didn't match the request\n",
    "                # Skip over it with pass.\n",
    "                if r is None:\n",
    "                    pass\n",
    "                \n",
    "                # If something is returned, take the match object's value and append\n",
    "                # the animal to the project directory.\n",
    "                else:\n",
    "                    print(project_dir.name, r.group(0))\n",
    "                    animal_list.append(project_dir / r.group(0))\n",
    "    \n",
    "    # Finally, return the list of animals\n",
    "    return animal_list\n",
    "\n",
    "animal_list = choose_animals(project_list, animal_group=\"E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c7f50cbd-7489-476a-bc55-90794f3092d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing...\n",
      "twop data\n",
      "\n",
      "From Projects(s)...\n",
      "learned_helplessness\n",
      "\n",
      "In Team(s)...\n",
      "specialk\n",
      "\n",
      "For Animals...\n",
      "LHE011\n",
      "LHE012\n",
      "LHE013\n",
      "LHE014\n",
      "LHE015\n",
      "LHE016\n",
      "\n",
      "Checking for directories...\n",
      "Found LHE011 twop\n",
      "Found LHE012 twop\n",
      "Found LHE013 twop\n",
      "Found LHE014 twop\n",
      "Found LHE015 twop\n",
      "Found LHE016 twop\n",
      "\n",
      "Returning Directories:\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE012\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE013\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE014\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE015\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE016\\twop\n"
     ]
    }
   ],
   "source": [
    "def choose_data(animal_list, data_group=[], verbose=True):\n",
    "    \"\"\"\n",
    "    Generates animal's data paths list based on user's selection.\n",
    "    \n",
    "    User can define which dataset to use for the animals they \n",
    "    want to use for their analyses. This function generates the\n",
    "    paths for their selection that meet specified conditions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of paths for animals of interest from choose_animals()\n",
    "    arg2: list\n",
    "        List of strings for which datasets to gather.\n",
    "        Default value is all.\n",
    "    arg3: bool\n",
    "        Boolean argument for verbose output of paths found or\n",
    "        not found by the function. Default is True.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team/project/animal/dataset Paths to grep in\n",
    "        next steps\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create empty data list for path generation\n",
    "    data_list = []\n",
    "    \n",
    "    # If data_group is left as default or specified as empty,\n",
    "    # grab all folders\n",
    "    if data_group == []:\n",
    "        print(\"Grabbing all data folders...\")\n",
    "        \n",
    "        # For each animal in the animal list\n",
    "        for animal in animal_list:\n",
    "            \n",
    "            # For the data_dir in the globbed animal_path\n",
    "            for data_dir in animal.glob(\"*\"):\n",
    "                \n",
    "                # Append the data_dir to the data_list\n",
    "                print(\"Grabbing\", animal.name, data_dir.name)\n",
    "                data_list.append(data)\n",
    "    \n",
    "    #TODO: Make verbose into its own function\n",
    "    elif len(data_group) > 0 and verbose is True:\n",
    "        print(\"Grabbing...\")\n",
    "        for data_type in data_group:\n",
    "            print(data_type, \"data\")\n",
    "\n",
    "        print(\"\\nFrom Projects(s)...\")\n",
    "        project_list = list(set([project.parent.name for project in animal_list]))\n",
    "        for project in project_list:\n",
    "            print(project)\n",
    "            \n",
    "        print(\"\\nIn Team(s)...\")\n",
    "        team_list = list(set([team.parent.parent.name for team in animal_list]))\n",
    "        for team in team_list:\n",
    "            print(team)\n",
    "        print(\"\\nFor Animals...\")\n",
    "        for animal in animal_list:\n",
    "            print(animal.name)\n",
    "        \n",
    "        print(\"\\nChecking for directories...\")\n",
    "        for animal in animal_list:\n",
    "            for data_type in data_group:\n",
    "                if (animal / data_type).is_dir():\n",
    "                    print(\"Found\", animal.name, data_type)\n",
    "                    data_list.append(animal / data_type)\n",
    "                else:\n",
    "                    print(\"Not Found!\", animal.name, data_type)\n",
    "    else:\n",
    "       \n",
    "        #TODO: Write a function for checking\n",
    "        print(\"Grabbing specified directories...\\n\")\n",
    "        \n",
    "        for animal in animal_list:\n",
    "            for data_type in data_group:\n",
    "                if (animal / data_type).is_dir():\n",
    "                    data_list.append(animal / data_type)\n",
    "                else:\n",
    "                    print(animal.name, data_type, \"Not Found!\")\n",
    "    \n",
    "    # Tell user which directories were returned\n",
    "    print(\"\\nReturning Directories:\")\n",
    "    for data_dir in data_list:\n",
    "        print(data_dir)\n",
    "\n",
    "    return data_list\n",
    "                \n",
    "\n",
    "data_list = choose_data(animal_list, data_group=[\"twop\"], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b9fb7173-b10f-43f4-a69d-0d9917e53076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grep_twop_behavior_raw(data_list):\n",
    "    \"\"\"\n",
    "    Generates animal's raw data paths list based on user's selection\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of paths for animals of interest from choose_data()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of 2P raw behavior data Paths\n",
    "    \"\"\"\n",
    "\n",
    "    twop_raw_beh_list = []\n",
    "    \n",
    "    \n",
    "    for directory in data_list:\n",
    "        search = directory.glob(\"*/*raw_behavior*/*.csv\")\n",
    "        for result in search:\n",
    "            twop_raw_beh_list.append(result)\n",
    "            \n",
    "    return twop_raw_beh_list\n",
    "\n",
    "twop_raw_beh_list = grep_twop_behavior_raw(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "bfc20455-f1b0-4bcf-a4da-e01cbb3bc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grep_twop_behavior_config(data_list):\n",
    "    \"\"\"\n",
    "    Generates animal's configuration paths list based on user's selection\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of paths for animals of interest from choose_data()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of 2P behavior configuration Paths\n",
    "    \"\"\"\n",
    "    \n",
    "    twop_config_list = []\n",
    "    \n",
    "    #TODO: Use .parents instead of parent.parent\n",
    "    for directory in data_list:\n",
    "        search = directory.glob(\"*/*.json\")\n",
    "        for result in search:\n",
    "            twop_config_list.append(result)\n",
    "            \n",
    "    return twop_config_list\n",
    "            \n",
    "twop_config_list = grep_twop_behavior_config(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "44b8c815-42bb-4ef4-9b90-75c69c8613c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210603\\20210603_LHE011_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210604\\20210604_LHE011_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210605\\20210605_LHE011_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210607\\20210607_LHE011_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210608\\20210608_LHE011_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210611\\20210611_LHE011_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210612\\20210612_LHE011_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210603\\20210603_LHE012_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210604\\20210604_LHE012_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210605\\20210605_LHE012_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210606\\20210606_LHE012_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210607\\20210607_LHE012_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210610\\20210610_LHE012_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210611\\20210611_LHE012_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210603\\20210603_LHE013_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210604\\20210604_LHE013_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210605\\20210605_LHE013_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210606\\20210606_LHE013_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210610\\20210610_LHE013_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210603\\20210603_LHE014_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210604\\20210604_LHE014_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210605\\20210605_LHE014_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210606\\20210606_LHE014_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210609\\20210609_LHE014_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210610\\20210610_LHE014_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210603\\20210603_LHE015_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210604\\20210604_LHE015_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210605\\20210605_LHE015_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210606\\20210606_LHE015_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210609\\20210609_LHE015_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210610\\20210610_LHE015_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210611\\20210611_LHE015_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210603\\20210603_LHE016_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210604\\20210604_LHE016_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210605\\20210605_LHE016_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210606\\20210606_LHE016_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210609\\20210609_LHE016_plane0_merged.json\n",
      "Session already merged: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210610\\20210610_LHE016_plane0_merged.json\n"
     ]
    }
   ],
   "source": [
    "def merge_2p_behavior(twop_raw_beh_list, twop_config_list, twop_microscopy_list=[]):\n",
    "    \"\"\"\n",
    "    Merges twop behavior and configuration files' data together\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of paths for raw behavior data\n",
    "    arg2: list\n",
    "        List of paths for configration files\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of merged 2P behavior paths\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Use Kyle's code to align these files with the relative timestamps of microscopy\n",
    "    # TODO: Get zipping of these two lists to work so finding config file is already completed...\n",
    "    # TODO: Make force overwrite/recompile the different datasets\n",
    "    # TODO: Make verbose version of this function\n",
    "    needs_merging = []\n",
    "    \n",
    "    merged_list = []\n",
    "    \n",
    "    # Give name_date pattern for the files we're aligning\n",
    "    name_date_pattern = re.compile(\"\\d{8}_[A-Z]{3}\\d{3}\")\n",
    "    \n",
    "    # First, check for cleaned data\n",
    "    for directory in twop_raw_beh_list:\n",
    "        \n",
    "        # Glob for the aligned json file\n",
    "        cleaned_check = directory.parents[1].glob(\"*_merged.json\")\n",
    "        \n",
    "        # Make a list using the result of the glob using list comprehension\n",
    "        clean_checklist = [session for session in cleaned_check]\n",
    "        \n",
    "        # If the list is empty, the session needs merging\n",
    "        if len(clean_checklist) == 0:\n",
    "            \n",
    "            # Show which session needs alignment and append the directory to needs_merging list\n",
    "            print(\"Session Needs Merging:\", directory.parents[3].name, directory.parents[1].name)\n",
    "            needs_merging.append(directory)\n",
    "        \n",
    "        # Else, the session has already been merged.  Append the filepath to the merged_list.\n",
    "        else:\n",
    "            merged_file = clean_checklist[0]\n",
    "            print(\"Session already merged:\", merged_file)\n",
    "            merged_list.append(merged_file)\n",
    "    \n",
    "    # Now, do the merging\n",
    "    for raw_file in needs_merging:\n",
    "        print(\"Merging:\", raw_file.name)\n",
    "        \n",
    "        merged = {}\n",
    "        \n",
    "        parent_folder = raw_file.parents[1]\n",
    "        \n",
    "        raw_behavior_df = pd.read_csv(raw_file, index_col=\"Time(ms)\").rename(columns=lambda col:col.strip())\n",
    "        \n",
    "        # Any value below 3V is not signal, turn it to zero by filtering\n",
    "        # values so all that remains are values greater than 3. All else\n",
    "        # will be 0.\n",
    "        raw_behavior_df = raw_behavior_df > 3\n",
    "        \n",
    "        # Convert all values to int; is necessary for pd.df.diff() to produce\n",
    "        # negative values used for stop times of each event\n",
    "        raw_behavior_df = raw_behavior_df.astype(int)\n",
    "        \n",
    "        # Take the diff of each column; gives start and stop of each signal\n",
    "        raw_behavior_df = raw_behavior_df.diff()\n",
    "\n",
    "        # Replace any NaN values with 0\n",
    "        raw_behavior_df = raw_behavior_df.fillna(0)\n",
    "        \n",
    "        # Grab start and stop values for licks\n",
    "        merged[\"LickOn\"] = raw_behavior_df[raw_behavior_df[\"Lick\"] == 1].index.tolist()\n",
    "        merged[\"LickOff\"] = raw_behavior_df[raw_behavior_df[\"Lick\"] == -1].index.tolist()\n",
    "        \n",
    "        # Grab start and stop values for Airpuff Solenoid\n",
    "        merged[\"AirpuffOn\"] = raw_behavior_df[raw_behavior_df[\"Airpuff\"] == 1].index.tolist()\n",
    "        merged[\"AirpuffOff\"] = raw_behavior_df[raw_behavior_df[\"Airpuff\"] == -1].index.tolist()\n",
    "        \n",
    "        # Grab start and stop values for Liquid Solenoid\n",
    "        merged[\"LiquidOn\"] = raw_behavior_df[raw_behavior_df[\"Liquid\"] == 1].index.tolist()\n",
    "        merged[\"LiquidOff\"] = raw_behavior_df[raw_behavior_df[\"Liquid\"] == -1].index.tolist()\n",
    "        \n",
    "        # Grab start and stop values for Speaker\n",
    "        merged[\"SpeakerOn\"] = raw_behavior_df[raw_behavior_df[\"Speaker\"] == 1].index.tolist()\n",
    "        merged[\"SpeakerOff\"] = raw_behavior_df[raw_behavior_df[\"Speaker\"] == -1].index.tolist()\n",
    "        \n",
    "        #TODO: This should be just one regex, not sure why complete isn't working...\n",
    "        \n",
    "        # Perform the regex for the name and date of the file\n",
    "        r_name_date = re.search(pattern=name_date_pattern, string=raw_file.name)\n",
    "        \n",
    "        # Give the pattern for the plane of interest\n",
    "        plane_pattern = \"_plane\\d{1}\"\n",
    "        \n",
    "        # Perform the regex for the plane number of the file\n",
    "        r_plane = re.search(pattern=plane_pattern, string=raw_file.name)\n",
    "        \n",
    "        # Concatenate strings into final merged file as json type\n",
    "        merged_name = r_name_date.group(0) + r_plane.group(0) + \"_merged.json\"\n",
    "        \n",
    "        # Append the parent folder with this name to create the file later\n",
    "        merged_filename = parent_folder / merged_name\n",
    "        \n",
    "        # Grab the config file for this plane from the parent folder\n",
    "        config_glob = parent_folder.glob(\"*.json\")\n",
    "        \n",
    "        # Config gather the config file result in a list\n",
    "        config_file_result = [config for config in config_glob]\n",
    "        \n",
    "        # The config file is the only element of this list, not sure how to retain only the relevant file without lists...\n",
    "        config_file = config_file_result[0]\n",
    "        \n",
    "        # Open the json file using json package \n",
    "        with open(config_file, \"r\") as inFile:\n",
    "            \n",
    "            # The configuration is the read file\n",
    "            config = inFile.read()\n",
    "            \n",
    "            # Load the contents of the config with json.loads()\n",
    "            config_contents = json.loads(config)\n",
    "            \n",
    "            # Gather the trial types from the configuration\n",
    "            trial_types = config_contents[\"trialArray\"]\n",
    "        \n",
    "        # Append trial types to aligned_file\n",
    "        merged[\"trialTypes\"] = trial_types\n",
    "\n",
    "        # Create the new file using json package\n",
    "        with open(merged_filename, \"w\") as outFile:\n",
    "            \n",
    "            # Use json.dump to write aligned_dictionary to file\n",
    "            json.dump(merged, outFile)\n",
    "        \n",
    "        # Tell user the file has been written\n",
    "        print(\"Written\", merged_filename)\n",
    "        aligned_list.append(merged_filename)\n",
    "        \n",
    "    return merged_list\n",
    "\n",
    "merged_list = merge_2p_behavior(twop_raw_beh_list, twop_config_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5722d9a1-ffdc-4492-a88a-b68ac60228f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210603\\20210603_LHE011_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210604\\20210604_LHE011_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210605\\20210605_LHE011_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210607\\20210607_LHE011_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210608\\20210608_LHE011_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210611\\20210611_LHE011_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210612\\20210612_LHE011_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210603\\20210603_LHE012_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210604\\20210604_LHE012_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210605\\20210605_LHE012_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210606\\20210606_LHE012_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210607\\20210607_LHE012_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210610\\20210610_LHE012_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210611\\20210611_LHE012_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210603\\20210603_LHE013_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210604\\20210604_LHE013_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210605\\20210605_LHE013_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210606\\20210606_LHE013_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210610\\20210610_LHE013_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210603\\20210603_LHE014_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210604\\20210604_LHE014_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210605\\20210605_LHE014_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210606\\20210606_LHE014_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210609\\20210609_LHE014_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210610\\20210610_LHE014_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210603\\20210603_LHE015_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210604\\20210604_LHE015_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210605\\20210605_LHE015_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210606\\20210606_LHE015_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210609\\20210609_LHE015_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210610\\20210610_LHE015_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210611\\20210611_LHE015_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210603\\20210603_LHE016_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210604\\20210604_LHE016_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210605\\20210605_LHE016_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210606\\20210606_LHE016_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210609\\20210609_LHE016_plane0_aligned_window5.csv\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210610\\20210610_LHE016_plane0_aligned_window5.csv\n"
     ]
    }
   ],
   "source": [
    "def align_2p_behavior(merged_list, alignment_positions=[\"LiquidOn\", \"SpeakerOn\"], window=5):\n",
    "    \"\"\"\n",
    "    Aligns twop behavior to different positions from user within specific window\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of paths for merged behavior data\n",
    "    arg2: list\n",
    "        Alignment positions to store\n",
    "    arg3: int\n",
    "        Window (in seconds) for pre/post event alignment\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of aligned 2P behavior paths\n",
    "    \"\"\"\n",
    "\n",
    "    needs_alignment = []\n",
    "    \n",
    "    aligned_list = []\n",
    "    \n",
    "    # First, check for cleaned data\n",
    "    for directory in merged_list:\n",
    "        \n",
    "        # Glob for the aligned json file\n",
    "        cleaned_check = directory.parents[0].glob(\"*aligned_*\")\n",
    "        \n",
    "        # Make a list using the result of the glob using list comprehension\n",
    "        clean_checklist = [session for session in cleaned_check]\n",
    "        \n",
    "        # If the list is empty, the session needs alignment\n",
    "        if len(clean_checklist) == 0:\n",
    "            \n",
    "            # Show which session needs alignment and append the directory to needs_merging list\n",
    "            print(\"Session Needs Alignment:\", directory.parents[2].name, directory.parents[0].name)\n",
    "            needs_alignment.append(directory)\n",
    "        \n",
    "        # Else, the session has already been aligned.  Append the filepath to the aligned_list.\n",
    "        else:\n",
    "            aligned_file = clean_checklist[0]\n",
    "            print(\"Session already aligned:\", aligned_file)\n",
    "            aligned_list.append(aligned_file)\n",
    "    \n",
    "    window = window * 1000\n",
    "\n",
    "    for merged_file in needs_alignment:\n",
    "\n",
    "        with open(merged_file, \"r\") as inFile:\n",
    "\n",
    "            contents = inFile.read()\n",
    "\n",
    "            timestamps = json.loads(contents)\n",
    "\n",
    "        trial_df = pd.DataFrame()\n",
    "        \n",
    "        lick_timestamps = []\n",
    "        liquid_counter = 0\n",
    "        airpuff_counter = 0\n",
    "\n",
    "        # TODO: Each segment should be its own function call ie liquidon, speakers, airpuffon\n",
    "        for (index, trial) in enumerate(timestamps[\"trialTypes\"]):\n",
    "            if trial == 1:\n",
    "                liquid_counter += 1\n",
    "                trial_name = \"Trial_\" + str(index + 1)\n",
    "                trial_type = \"Sucrose_\" + str(liquid_counter)\n",
    "                s = pd.Series(trial_type, dtype=str, name=trial_name)\n",
    "                trial_df = trial_df.append(s)\n",
    "            else:\n",
    "                airpuff_counter += 1\n",
    "                trial_name = \"Trial_\" + str(index + 1)\n",
    "                trial_type = \"Airpuff_\" + str(airpuff_counter)\n",
    "                s = pd.Series(trial_type, dtype=str, name=trial_name)\n",
    "                trial_df = trial_df.append(s)\n",
    "        \n",
    "        trial_df.index.name = \"trial\"\n",
    "        trial_df.columns = [\"trial_type\"]\n",
    "        \n",
    "        exclude_keys = [\"LickOn\", \"LickOff\", \"LiquidOff\", \"AirpuffOff\", \"trialTypes\"]\n",
    "        \n",
    "        for key in timestamps.keys():\n",
    "            if key not in exclude_keys:\n",
    "                trial_df[key] = np.nan\n",
    "        \n",
    "        speakeron_df = pd.DataFrame()\n",
    "        \n",
    "        for (index, trial) in enumerate(timestamps[\"SpeakerOn\"]):\n",
    "            trial_number = \"Trial_\" + str(index + 1)\n",
    "            speaker_on_ms = trial\n",
    "            s = pd.Series(speaker_on_ms, name=trial_number)\n",
    "            speakeron_df = speakeron_df.append(s)\n",
    "        \n",
    "        speakeron_df.index.name = \"trial\"\n",
    "        speakeron_df.columns = [\"SpeakerOn\"]\n",
    "        \n",
    "        trial_df.update(speakeron_df)\n",
    "        \n",
    "        speakeroff_df = pd.DataFrame()\n",
    "        \n",
    "        for (index, trial) in enumerate(timestamps[\"SpeakerOff\"]):\n",
    "            \n",
    "            trial_number = \"Trial_\" + str(index + 1)\n",
    "            speaker_off_ms = trial\n",
    "            s = pd.Series(speaker_off_ms, dtype=int, name=trial_number)\n",
    "            speakeroff_df = speakeroff_df.append(s)\n",
    "        \n",
    "        speakeroff_df.index.name = \"trial\"\n",
    "        speakeroff_df.columns = [\"SpeakerOff\"]\n",
    "        \n",
    "        trial_df.update(speakeroff_df)\n",
    "        \n",
    "        trial_df.reset_index(inplace=True)\n",
    "        trial_df.set_index(\"trial_type\", inplace=True)\n",
    "\n",
    "        \n",
    "        liquid_df = pd.DataFrame()\n",
    "\n",
    "        for (index, trial) in enumerate(timestamps[\"LiquidOn\"]):\n",
    "            \n",
    "            liquid_trial = \"Sucrose_\" + str(index + 1)\n",
    "            liquid_start_ms = trial\n",
    "            s = pd.Series(liquid_start_ms, dtype=int, name=liquid_trial)\n",
    "            liquid_df = liquid_df.append(s)\n",
    "        \n",
    "        liquid_df.index.name = \"trial_type\"\n",
    "        liquid_df.columns = [\"LiquidOn\"]\n",
    "        \n",
    "        trial_df.update(liquid_df)\n",
    "        \n",
    "        airpuff_df = pd.DataFrame()\n",
    "\n",
    "        for (index, trial) in enumerate(timestamps[\"AirpuffOn\"]):\n",
    "            airpuff_trial = \"Airpuff_\" + str(index + 1)\n",
    "            airpuff_start_ms = trial\n",
    "            s = pd.Series(airpuff_start_ms, dtype=int, name=airpuff_trial)\n",
    "            airpuff_df = airpuff_df.append(s)\n",
    "        \n",
    "        airpuff_df.index.name = \"trial_type\"\n",
    "        \n",
    "        if airpuff_df.size > 0:\n",
    "            airpuff_df.columns = [\"AirpuffOn\"]\n",
    "            trial_df.update(airpuff_df)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        trial_df.reset_index(inplace=True)\n",
    "        trial_df.set_index(\"trial\", inplace=True)\n",
    "\n",
    "                \n",
    "        # Use list comprehension for lick timestamps\n",
    "        lick_timestamps = [lick for (index, lick) in enumerate(timestamps[\"LickOn\"])]\n",
    "        \n",
    "        # Create columns for aligning licks to different events and then create lick lists\n",
    "        # This is terrible and a better way to assign licks to a list should be found, but\n",
    "        # without using a relational system I'm not sure how I can alter this...\n",
    "        # TODO: Make this a function that is iterated over instead of multiple for loops like this...\n",
    "        # Possible solutions: Create ITI starts/stops as dataframe values and evaluate against range\n",
    "        \n",
    "        for target in alignment_positions:\n",
    "            column = target + \"_licks\"\n",
    "            trial_df[column] = np.empty((len(trial_df), 0)).tolist()\n",
    "            for lick_time in lick_timestamps:\n",
    "                for value in trial_df[target].items():\n",
    "                    if (value[1] - window) <= lick_time <= (value[1] + window):\n",
    "                        tmp = value[0]\n",
    "                        trial_df[column].loc[tmp].append(lick_time)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        # Create empty lists for ITI licks\n",
    "        trial_df[\"ITI_licks\"] = np.empty((len(trial_df), 0)).tolist()\n",
    "\n",
    "        # Get leftover ITI licks by creating sets out of aligned licks\n",
    "        accounted_licks = []\n",
    "        for value in trial_df[\"LiquidOn_licks\"].items():\n",
    "            if len(value[1]) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                for licks in value[1]:\n",
    "                    accounted_licks.append(licks)\n",
    "        \n",
    "        accounted_licks = set(accounted_licks)\n",
    "        licks_set = set(lick_timestamps) \n",
    "        iti_licks = list(licks_set - accounted_licks)\n",
    "\n",
    "\n",
    "        # Again, a bad way to do this. Too many loops and likely to be quite slow...\n",
    "        for lick_time in iti_licks:\n",
    "            for value in trial_df[\"ITI_licks\"].items():\n",
    "                tmp = value[0]\n",
    "                tmp2 = value[1]\n",
    "                if (trial_df[\"SpeakerOn\"].loc[tmp] - window) <= lick_time <= (trial_df[\"LiquidOn\"].loc[tmp] + window):\n",
    "                    trial_df[\"ITI_licks\"].loc[tmp].append(lick_time)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # DEPRECATED: Deprecated for now unless told that having precise timings for ITIs is necessary\n",
    "        # NOTE: Unsure this is necessary, should be able to infer the ITIs without explicitly having\n",
    "        # having start/stop times for ITIs\n",
    "#         trial_df[\"ITI_start\"] = np.nan\n",
    "#         trial_df[\"ITI_end\"] = np.nan\n",
    "\n",
    "        # Uses .loc for the index and then the column of interest to set actual value of frame\n",
    "        # not a copy of the dataframe\n",
    "        # Set first ITI start as 0ms\n",
    "#         trial_df.loc[\"Trial_1\", \"ITI_start\"] = 0\n",
    "        \n",
    "#         # Set first ITI end as first SpeakerOn value\n",
    "#         trial_df.loc[\"Trial_1\", \"ITI_end\"] = trial_df.loc[\"Trial_1\", \"SpeakerOn\"]\n",
    "        \n",
    "#         trial_df.loc[1:len(trial_df), \"ITI_start\"] = trial_df.LiquidOn[1:len(trial_df)-1]\n",
    "#         trial_df.loc[1:len(trial_df), \"ITI_end\"] = trial_df.SpeakerOn[1:len(trial_df)]\n",
    "\n",
    "        aligned_filename = merged_file.stem\n",
    "        aligned_filename = aligned_filename.replace(\"merged\", \"aligned\")\n",
    "        aligned_filename = aligned_filename + \"_\" + \"window\" + str(int(window/1000)) + \".csv\"\n",
    "        \n",
    "        aligned_file_path = merged_file.parents[0] / aligned_filename\n",
    "        \n",
    "        aligned_list.append(aligned_file_path)\n",
    "        \n",
    "        trial_df.to_csv(aligned_file_path)\n",
    "\n",
    "    return aligned_list\n",
    "\n",
    "\n",
    "aligned_list = align_2p_behavior(merged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e9690-3517-4b65-9483-f73e3a34ff0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
