{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e79f5a-afdd-4b77-a0cc-e9a072fb697f",
   "metadata": {},
   "source": [
    "# Data Grepping Notebook\n",
    "### Jeremy Delahanty June 2021\n",
    "\n",
    "Intended to grep different files/projects/datasets from user input and retain them for use in analysis/display later. The lack of unified filenaming structures between projects will break the code... A convetion of XXX### for animal names, or something similar, should be adopted for all animals in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "236756af-545f-4c82-bfc7-7f22a45d8a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def109c7-4a96-41a8-b0c5-a66ab01b0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_basepath = \"Y:/\"\n",
    "# project_dict = {\"specialk\": [\"learned_helplessness\", \"chronic_mild_stress\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "62578e70-c52a-4a7e-9d62-b7c5d0b02268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found All Selected Teams\n",
      "Teams Returned:\n",
      "specialk \n"
     ]
    }
   ],
   "source": [
    "def grep_teams(team_selection=[], lab_basepath=\"Y:/\"):\n",
    "    \"\"\"\n",
    "    Grabs team list from server based on user's input.\n",
    "    \n",
    "    User can define which teams they want to use for their analyses and\n",
    "    the function will glob the paths for their selection.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of strings for teams of interest\n",
    "        Default is empty list\n",
    "    arg2: string\n",
    "        Basepath for server location on machine\n",
    "        Default is Y:/ for mapped Windows drive\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team path grabbed from server successfully\n",
    "    2. list\n",
    "        List of teams not found\n",
    "    \"\"\"\n",
    "\n",
    "    # Take basepath and glob all available files and directories\n",
    "    team_glob = Path(lab_basepath).glob(\"{}\".format(\"*\"))\n",
    "    \n",
    "    # Check if no team was specifically asked for, tell user we're gathering all teams\n",
    "    if team_selection == []:\n",
    "\n",
    "        print(\"Gathering all teams...\")\n",
    "\n",
    "        # List comprehension for returning all directories in Tye Lab server\n",
    "        team_list = [team for team in team_glob if team.is_dir()]\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # List comprehension for returning only directories user wants in the Tye Lab server \n",
    "        team_list = [team for team in team_glob if team.name in team_selection and team.is_dir()]\n",
    "    \n",
    "    # Create temporary list for checking if selected teams exist\n",
    "    tmp = []\n",
    "\n",
    "    # For the teams that were globbed successfully, append the team to the temp list\n",
    "    for globbed_team in team_list:\n",
    "        tmp.append(globbed_team.name)\n",
    "    \n",
    "    # Compare team selection with returned teams using sets, convert to list\n",
    "    missing_teams = list(set(team_selection) - set(tmp))\n",
    "    \n",
    "    # If the missing_teams list is empty, the program found all requested teams\n",
    "    if missing_teams == []:\n",
    "        print(\"Found All Selected Teams\")\n",
    "    \n",
    "    # Else, some teams weren't found. Tell the user which teams weren't found.\n",
    "    else:\n",
    "        print(\"Failed to find team(s):\", missing_teams)\n",
    "   \n",
    "    # Show user which teams were returned\n",
    "    print(\"Teams Returned:\")\n",
    "    for team in team_list:\n",
    "        print(\"{} \".format(team.name))\n",
    "    \n",
    "    # Return the list of projects gathered\n",
    "    return team_list, missing_teams\n",
    "\n",
    "team_list, missing_teams = grep_teams([\"specialk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "d47d606e-6e66-4ffb-a644-1af2834b8e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned Directories: \n",
      "Y:\\specialk\\learned_helplessness\n"
     ]
    }
   ],
   "source": [
    "def choose_projects(team_list, project_selection={}):\n",
    "    \"\"\"\n",
    "    Generates project paths list based on user's selection.\n",
    "    \n",
    "    User can define which project they want to use for their analyses and\n",
    "    this function generates the paths for their selection.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of strings for teams of interest from grep_teams()\n",
    "    arg2: dict\n",
    "        Dictionary of values that will be used to create specific\n",
    "        paths for selected teams and their projects\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team/project Paths to grep in next steps\n",
    "    \"\"\"\n",
    "\n",
    "    # Make dictionary using the teams in team_list as keys\n",
    "    project_dict = {team: [] for team in team_list}\n",
    "\n",
    "    # For each time in the team_list, append the Path name's project's values\n",
    "    for team in team_list:\n",
    "        project_dict[team].append(project_selection[team.name])\n",
    "    \n",
    "    # Make empty project list\n",
    "    project_dir_list = []\n",
    "\n",
    "    for team in project_dict.keys():\n",
    "        for project in range(len(project_dict[team][0])):\n",
    "            project_dir_list.append(team / project_dict[team][0][project])\n",
    "    \n",
    "    print(\"Returned Directories: \")\n",
    "\n",
    "    for directory in project_dir_list:\n",
    "        print(directory)\n",
    "    \n",
    "    return project_dir_list\n",
    "    \n",
    "project_list = choose_projects(team_list, project_selection={\"specialk\": [\"learned_helplessness\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "d6c423b7-92e8-45ba-9af8-4103534166d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing only E animals...\n",
      "learned_helplessness LHE011\n",
      "learned_helplessness LHE012\n",
      "learned_helplessness LHE013\n",
      "learned_helplessness LHE014\n",
      "learned_helplessness LHE015\n",
      "learned_helplessness LHE016\n"
     ]
    }
   ],
   "source": [
    "def choose_animals(project_list, animal_group=\"all\"):\n",
    "    \"\"\"\n",
    "    Generates animal paths list based on user's selection.\n",
    "    \n",
    "    User can define which cohort of animals they want to use \n",
    "    for their analyses. This function generates the paths for \n",
    "    their selection that meet specified conditions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of strings for projects of interest from choose_projects()\n",
    "    arg2: str\n",
    "        String of value for which animal paths to gather.\n",
    "        Default value is all.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team/project/animal Paths to grep in next steps\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create empty animal list for path generation\n",
    "    animal_list = []\n",
    "    \n",
    "    # If the animal group is left as default/specified as all, grab all animals\n",
    "    if animal_group == \"all\":\n",
    "        print(\"Grabbing all animals...\")\n",
    "        \n",
    "        # For each project directory in the project list\n",
    "        for project_dir in project_list:\n",
    "            \n",
    "            # For each animal globbed in the project directory\n",
    "            for animal in project_dir.glob(\"*\"):\n",
    "                \n",
    "                # Append the animal's path to the animal_list\n",
    "                print(project_dir.name, animal.name)\n",
    "                animal_list.append(animal)\n",
    "    \n",
    "    # Else, only select animals from the specified group\n",
    "    else:\n",
    "        print(\"Grabbing only {} animals...\".format(animal_group))\n",
    "\n",
    "        # Format the animal group with the user's input\n",
    "        animal_group = \"[A-Z]{2}\" + animal_group + \"\\d{3}\"\n",
    "        \n",
    "        # For each project_directory in project_list\n",
    "        for project_dir in project_list:\n",
    "\n",
    "            # For each animal globbed in project directory\n",
    "            for animal in project_dir.glob(\"*\"):\n",
    "                \n",
    "                # Use regex to grab only the requested animal\n",
    "                r = re.search(animal_group, string=animal.name)\n",
    "                \n",
    "                # If the search returns None, the animal didn't match the request\n",
    "                # Skip over it with pass.\n",
    "                if r is None:\n",
    "                    pass\n",
    "                \n",
    "                # If something is returned, take the match object's value and append\n",
    "                # the animal to the project directory.\n",
    "                else:\n",
    "                    print(project_dir.name, r.group(0))\n",
    "                    animal_list.append(project_dir / r.group(0))\n",
    "    \n",
    "    # Finally, return the list of animals\n",
    "    return animal_list\n",
    "\n",
    "animal_list = choose_animals(project_list, animal_group=\"E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "c7f50cbd-7489-476a-bc55-90794f3092d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing specified directories...\n",
      "\n",
      "\n",
      "Returning Directories:\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE012\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE013\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE014\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE015\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE016\\twop\n"
     ]
    }
   ],
   "source": [
    "def choose_data(animal_list, data_group=[], verbose=True):\n",
    "    \"\"\"\n",
    "    Generates animal's data paths list based on user's selection.\n",
    "    \n",
    "    User can define which dataset to use for the animals they \n",
    "    want to use for their analyses. This function generates the\n",
    "    paths for their selection that meet specified conditions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of paths for animals of interest from choose_animals()\n",
    "    arg2: list\n",
    "        List of strings for which datasets to gather.\n",
    "        Default value is all.\n",
    "    arg3: bool\n",
    "        Boolean argument for verbose output of paths found or\n",
    "        not found by the function. Default is True.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team/project/animal/dataset Paths to grep in\n",
    "        next steps\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create empty data list for path generation\n",
    "    data_list = []\n",
    "    \n",
    "    # If data_group is left as default or specified as empty,\n",
    "    # grab all folders\n",
    "    if data_group == []:\n",
    "        print(\"Grabbing all data folders...\")\n",
    "        \n",
    "        # For each animal in the animal list\n",
    "        for animal in animal_list:\n",
    "            \n",
    "            # For the data_dir in the globbed animal_path\n",
    "            for data_dir in animal.glob(\"*\"):\n",
    "                \n",
    "                # Append the data_dir to the data_list\n",
    "                print(\"Grabbing\", animal.name, data_dir.name)\n",
    "                data_list.append(data)\n",
    "    \n",
    "    #TODO: Make verbose into its own function\n",
    "    elif len(data_group) > 0 and verbose is True:\n",
    "        print(\"Grabbing...\")\n",
    "        for data_type in data_group:\n",
    "            print(data_type, \"data\")\n",
    "\n",
    "        print(\"\\nFrom Projects(s)...\")\n",
    "        project_list = list(set([project.parent.name for project in animal_list]))\n",
    "        for project in project_list:\n",
    "            print(project)\n",
    "            \n",
    "        print(\"\\nIn Team(s)...\")\n",
    "        team_list = list(set([team.parent.parent.name for team in animal_list]))\n",
    "        for team in team_list:\n",
    "            print(team)\n",
    "        print(\"\\nFor Animals...\")\n",
    "        for animal in animal_list:\n",
    "            print(animal.name)\n",
    "        \n",
    "        print(\"\\nChecking for directories...\")\n",
    "        for animal in animal_list:\n",
    "            for data_type in data_group:\n",
    "                if (animal / data_type).is_dir():\n",
    "                    print(\"Found\", animal.name, data_type)\n",
    "                    data_list.append(animal / data_type)\n",
    "                else:\n",
    "                    print(\"Not Found!\", animal.name, data_type)\n",
    "    else:\n",
    "       \n",
    "        #TODO: Write a function for checking\n",
    "        print(\"Grabbing specified directories...\\n\")\n",
    "        \n",
    "        for animal in animal_list:\n",
    "            for data_type in data_group:\n",
    "                if (animal / data_type).is_dir():\n",
    "                    data_list.append(animal / data_type)\n",
    "                else:\n",
    "                    print(animal.name, data_type, \"Not Found!\")\n",
    "    \n",
    "    # Tell user which directories were returned\n",
    "    print(\"\\nReturning Directories:\")\n",
    "    for data_dir in data_list:\n",
    "        print(data_dir)\n",
    "\n",
    "    return data_list\n",
    "                \n",
    "\n",
    "data_list = choose_data(animal_list, data_group=[\"twop\"], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "b9fb7173-b10f-43f4-a69d-0d9917e53076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grep_twop_behavior_raw(data_list, session_type=[]):\n",
    "\n",
    "    twop_raw_beh_list = []\n",
    "    \n",
    "    \n",
    "    for directory in data_list:\n",
    "        all_search = directory.glob(\"*/*raw_behavior*/*.csv\")\n",
    "        for result in all_search:\n",
    "            twop_raw_beh_list.append(result)\n",
    "            \n",
    "    return twop_raw_beh_list\n",
    "\n",
    "twop_raw_beh_list = grep_twop_behavior_raw(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "bfc20455-f1b0-4bcf-a4da-e01cbb3bc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grep_twop_behavior_config(twop_raw_beh_list):\n",
    "    \n",
    "    twop_config_list = []\n",
    "    \n",
    "    #TODO: Use .parents instead of parent.parent\n",
    "    for directory in twop_raw_beh_list:\n",
    "        search = directory.parent.parent.glob(\"*.json\")\n",
    "        for result in search:\n",
    "            twop_config_list.append(result)\n",
    "            \n",
    "    return twop_config_list\n",
    "            \n",
    "twop_config_list = grep_twop_behavior_config(twop_raw_beh_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "44b8c815-42bb-4ef4-9b90-75c69c8613c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210603\\20210603_LHE011_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210604\\20210604_LHE011_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210605\\20210605_LHE011_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210607\\20210607_LHE011_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210608\\20210608_LHE011_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210611\\20210611_LHE011_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210612\\20210612_LHE011_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210603\\20210603_LHE012_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210604\\20210604_LHE012_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210605\\20210605_LHE012_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210606\\20210606_LHE012_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210607\\20210607_LHE012_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210610\\20210610_LHE012_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210611\\20210611_LHE012_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210603\\20210603_LHE013_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210604\\20210604_LHE013_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210605\\20210605_LHE013_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210606\\20210606_LHE013_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210610\\20210610_LHE013_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210603\\20210603_LHE014_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210604\\20210604_LHE014_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210605\\20210605_LHE014_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210606\\20210606_LHE014_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210609\\20210609_LHE014_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210610\\20210610_LHE014_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210603\\20210603_LHE015_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210604\\20210604_LHE015_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210605\\20210605_LHE015_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210606\\20210606_LHE015_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210609\\20210609_LHE015_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210610\\20210610_LHE015_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210611\\20210611_LHE015_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210603\\20210603_LHE016_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210604\\20210604_LHE016_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210605\\20210605_LHE016_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210606\\20210606_LHE016_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210609\\20210609_LHE016_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210610\\20210610_LHE016_plane0_aligned.json\n"
     ]
    }
   ],
   "source": [
    "def align_2p_behavior(twop_raw_beh_list, twop_config_list, twop_microscopy_list=[]):\n",
    "    \n",
    "    # TODO: Use Kyle's code to align these files with the relative timestamps of microscopy\n",
    "    # TODO: Get zipping of these two lists to work so finding config file is already completed...\n",
    "    # TODO: Make force overwrite/recompile the different datasets\n",
    "    # TODO: Make verbose version of this function\n",
    "    needs_alignment = []\n",
    "    \n",
    "    aligned_list = []\n",
    "    \n",
    "    # First, check for cleaned data\n",
    "    for directory in twop_raw_beh_list:\n",
    "        \n",
    "        # Glob for the aligned json file\n",
    "        cleaned_check = directory.parents[1].glob(\"*_aligned.json\")\n",
    "        \n",
    "        # Make a list using the result of the glob using list comprehension\n",
    "        clean_checklist = [session for session in cleaned_check]\n",
    "        \n",
    "        # If the list is empty, the session needs alignment\n",
    "        if len(clean_checklist) == 0:\n",
    "            \n",
    "            # Show which session needs alignment and append the directory to needs_alignment list\n",
    "            print(\"Session Needs Alignment:\", directory.parents[3].name, directory.parents[1].name)\n",
    "            needs_alignment.append(directory)\n",
    "        \n",
    "        # Else, the session has already been aligned.  Append the filepath to the aligned_list.\n",
    "        else:\n",
    "            aligned_file = clean_checklist[0]\n",
    "            print(\"Session already aligned:\", aligned_file)\n",
    "            aligned_list.append(aligned_file)\n",
    "    \n",
    "    # Now, do the cleaning\n",
    "    \n",
    "    for raw_file in needs_alignment:\n",
    "        print(\"Aligning:\", raw_file.name)\n",
    "        \n",
    "        aligned = {}\n",
    "        \n",
    "        parent_folder = raw_file.parents[1]\n",
    "        \n",
    "        raw_behavior_df = pd.read_csv(raw_file, index_col=\"Time(ms)\").rename(columns=lambda col:col.strip())\n",
    "        \n",
    "        # Any value below 3V is not signal, turn it to zero by filtering\n",
    "        # values so all that remains are values greater than 3. All else\n",
    "        # will be 0.\n",
    "        raw_behavior_df = raw_behavior_df > 3\n",
    "        \n",
    "        # Convert all values to int; is necessary for pd.df.diff() to produce\n",
    "        # negative values used for stop times of each event\n",
    "        raw_behavior_df = raw_behavior_df.astype(int)\n",
    "        \n",
    "        # Take the diff of each column; gives start and stop of each signal\n",
    "        raw_behavior_df = raw_behavior_df.diff()\n",
    "\n",
    "        # Replace any NaN values with 0\n",
    "        raw_behavior_df = raw_behavior_df.fillna(0)\n",
    "        \n",
    "        # Grab start and stop values for licks\n",
    "        aligned[\"LickOn\"] = raw_behavior_df[raw_behavior_df[\"Lick\"] == 1].index.tolist()\n",
    "        aligned[\"LickOff\"] = raw_behavior_df[raw_behavior_df[\"Lick\"] == -1].index.tolist()\n",
    "        \n",
    "        # Grab start and stop values for Airpuff Solenoid\n",
    "        aligned[\"AirpuffOn\"] = raw_behavior_df[raw_behavior_df[\"Airpuff\"] == 1].index.tolist()\n",
    "        aligned[\"AirpuffOff\"] = raw_behavior_df[raw_behavior_df[\"Airpuff\"] == -1].index.tolist()\n",
    "        \n",
    "        # Grab start and stop values for Liquid Solenoid\n",
    "        aligned[\"LiquidOn\"] = raw_behavior_df[raw_behavior_df[\"Liquid\"] == 1].index.tolist()\n",
    "        aligned[\"LiquidOff\"] = raw_behavior_df[raw_behavior_df[\"Liquid\"] == -1].index.tolist()\n",
    "        \n",
    "        # Grab start and stop values for Speaker\n",
    "        aligned[\"SpeakerOn\"] = raw_behavior_df[raw_behavior_df[\"Speaker\"] == 1].index.tolist()\n",
    "        aligned[\"SpeakerOff\"] = raw_behavior_df[raw_behavior_df[\"Speaker\"] == -1].index.tolist()\n",
    "        \n",
    "        #TODO: This should be just one regex, not sure why complete isn't working...\n",
    "        # Give name_date pattern for the files we're aligning\n",
    "        name_date_pattern = \"\\d{8}_[A-Z]{3}\\d{3}\"\n",
    "        \n",
    "        # Perform the regex for the name and date of the file\n",
    "        r_name_date = re.search(pattern=name_date_pattern, string=raw_file.name)\n",
    "        \n",
    "        # Give the pattern for the plane of interest\n",
    "        plane_pattern = \"_plane\\d{1}\"\n",
    "        \n",
    "        # Perform the regex for the plane number of the file\n",
    "        r_plane = re.search(pattern=plane_pattern, string=raw_file.name)\n",
    "        \n",
    "        # Concatenate strings into final aligned file as json type\n",
    "        aligned_name = r_name_date.group(0) + r_plane.group(0) + \"_aligned.json\"\n",
    "        \n",
    "        # Append the parent folder with this name to create the file later\n",
    "        aligned_filename = parent_folder / aligned_name\n",
    "        \n",
    "        # Grab the config file for this plane from the parent folder\n",
    "        config_glob = parent_folder.glob(\"*.json\")\n",
    "        \n",
    "        # Config gather the config file result in a list\n",
    "        config_file_result = [config for config in config_glob]\n",
    "        \n",
    "        # The config file is the only element of this list, not sure how to retain only the relevant file without lists...\n",
    "        config_file = config_file_result[0]\n",
    "        \n",
    "        # Open the json file using json package \n",
    "        with open(config_file, \"r\") as inFile:\n",
    "            \n",
    "            # The configuration is the read file\n",
    "            config = inFile.read()\n",
    "            \n",
    "            # Load the contents of the config with json.loads()\n",
    "            config_contents = json.loads(config)\n",
    "            \n",
    "            # Gather the trial types from the configuration\n",
    "            trial_types = config_contents[\"trialArray\"]\n",
    "        \n",
    "        # Append trial types to aligned_file\n",
    "        aligned[\"trialTypes\"] = trial_types\n",
    "\n",
    "        # Create the new file using json package\n",
    "        with open(aligned_filename, \"w\") as outFile:\n",
    "            \n",
    "            # Use json.dump to write aligned_dictionary to file\n",
    "            json.dump(aligned, outFile)\n",
    "        \n",
    "        # Tell user the file has been written\n",
    "        print(\"Written\", aligned_filename)\n",
    "        aligned_list.append(aligned_filename)\n",
    "        \n",
    "    return aligned_list\n",
    "\n",
    "aligned_list = align_2p_behavior(twop_raw_beh_list, twop_config_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "5722d9a1-ffdc-4492-a88a-b68ac60228f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          trial_type  AirpuffOn   LiquidOn  SpeakerOn  SpeakerOff  Licks\n",
      "trial                                                                   \n",
      "Trial_1    Sucrose_1        NaN    31267.0    28979.0     31267.0    NaN\n",
      "Trial_2    Sucrose_2        NaN    62181.0    60437.0     62180.0    NaN\n",
      "Trial_3    Sucrose_3        NaN    98013.0    95535.0     98013.0    NaN\n",
      "Trial_4    Sucrose_4        NaN   131047.0   129676.0    131047.0    NaN\n",
      "Trial_5    Sucrose_5        NaN   160455.0   158009.0    160455.0    NaN\n",
      "Trial_6    Sucrose_6        NaN   189489.0   185199.0    189489.0    NaN\n",
      "Trial_7    Sucrose_7        NaN   220157.0   215185.0    220156.0    NaN\n",
      "Trial_8    Sucrose_8        NaN   250916.0   249565.0    250915.0    NaN\n",
      "Trial_9    Sucrose_9        NaN   276457.0   275249.0    276457.0    NaN\n",
      "Trial_10  Sucrose_10        NaN   304317.0   302795.0    304317.0    NaN\n",
      "Trial_11  Sucrose_11        NaN   336235.0   335209.0    336234.0    NaN\n",
      "Trial_12  Sucrose_12        NaN   371498.0   368739.0    371498.0    NaN\n",
      "Trial_13  Sucrose_13        NaN   400687.0   396611.0    400687.0    NaN\n",
      "Trial_14  Sucrose_14        NaN   423253.0   420409.0    423253.0    NaN\n",
      "Trial_15  Sucrose_15        NaN   456802.0   453294.0    456802.0    NaN\n",
      "Trial_16  Sucrose_16        NaN   491662.0   487378.0    491662.0    NaN\n",
      "Trial_17  Sucrose_17        NaN   514411.0   511288.0    514411.0    NaN\n",
      "Trial_18  Sucrose_18        NaN   541820.0   540292.0    541820.0    NaN\n",
      "Trial_19  Sucrose_19        NaN   577506.0   573189.0    577506.0    NaN\n",
      "Trial_20  Sucrose_20        NaN   603959.0   600153.0    603959.0    NaN\n",
      "Trial_21  Sucrose_21        NaN   627615.0   626204.0    627615.0    NaN\n",
      "Trial_22  Sucrose_22        NaN   663179.0   659879.0    663179.0    NaN\n",
      "Trial_23  Sucrose_23        NaN   688895.0   685867.0    688895.0    NaN\n",
      "Trial_24  Sucrose_24        NaN   717238.0   714578.0    717238.0    NaN\n",
      "Trial_25  Sucrose_25        NaN   750489.0   748792.0    750488.0    NaN\n",
      "Trial_26  Sucrose_26        NaN   775726.0   772654.0    775726.0    NaN\n",
      "Trial_27  Sucrose_27        NaN   801425.0   800008.0    801425.0    NaN\n",
      "Trial_28  Sucrose_28        NaN   834778.0   831439.0    834778.0    NaN\n",
      "Trial_29  Sucrose_29        NaN   864133.0   859821.0    864133.0    NaN\n",
      "Trial_30  Sucrose_30        NaN   886330.0   882906.0    886330.0    NaN\n",
      "Trial_31  Sucrose_31        NaN   914829.0   910268.0    914829.0    NaN\n",
      "Trial_32  Sucrose_32        NaN   940982.0   937162.0    940982.0    NaN\n",
      "Trial_33  Sucrose_33        NaN   966892.0   965733.0    966892.0    NaN\n",
      "Trial_34  Sucrose_34        NaN   990637.0   987189.0    990637.0    NaN\n",
      "Trial_35  Sucrose_35        NaN  1021488.0  1018545.0   1021487.0    NaN\n",
      "Trial_36  Sucrose_36        NaN  1046749.0  1042506.0   1046749.0    NaN\n",
      "Trial_37  Sucrose_37        NaN  1080284.0  1078470.0   1080283.0    NaN\n",
      "Trial_38  Sucrose_38        NaN  1109809.0  1107115.0   1109809.0    NaN\n",
      "Trial_39  Sucrose_39        NaN  1138649.0  1136555.0   1138648.0    NaN\n",
      "Trial_40  Sucrose_40        NaN  1165657.0  1162198.0   1165657.0    NaN\n",
      "[16179, 16274, 22613, 32804, 33192, 33299, 33417, 33554, 33679, 33808, 33949, 34086, 34246, 34425, 34604, 34969, 35924, 42886, 64981, 65342, 65453, 65582, 65708, 65856, 66004, 66811, 67096, 67275, 67499, 67731, 71787, 71905, 72327, 72631, 80468, 86208, 86246, 98485, 98774, 98877, 99010, 99154, 99360, 99561, 99729, 99919, 100380, 105378, 110241, 118017, 133059, 133196, 133455, 133554, 133668, 133794, 133931, 134101, 134243, 134398, 134699, 135555, 136248, 137960, 138078, 138291, 138470, 142191, 149527, 152829, 157478, 158570, 164097, 208305, 215464, 215707, 223581, 223718, 234105, 240244, 244185, 244383, 249818, 274661, 276745, 277233, 277342, 277453, 277579, 277731, 277864, 278009, 278195, 278366, 278545, 279036, 279390, 279789, 280040, 280269, 281322, 287820, 297016, 304701, 305085, 305180, 305298, 305458, 305602, 305755, 305922, 307641, 307919, 309658, 310073, 310133, 310411, 330392, 333284, 338622, 338892, 339094, 339204, 339311, 339436, 339558, 339687, 339805, 339939, 340102, 340353, 340654, 341997, 342259, 344132, 349256, 352573, 355031, 368971, 369515, 371885, 372251, 372460, 372578, 372703, 372874, 373137, 373327, 374610, 375104, 377087, 377364, 402081, 402774, 403132, 403258, 403383, 403623, 404019, 409871, 420628, 426226, 427097, 427268, 427565, 427824, 427991, 430526, 430971, 434217, 434574, 443654, 445998, 448141, 457639, 457856, 457970, 458092, 458336, 492488, 493044, 493158, 493527, 493660, 493801, 493953, 494162, 494962, 495099, 495738, 501676, 503677, 514883, 515697, 515842, 516001, 517733, 517858, 518045, 518208, 544466, 562539, 567757, 573443, 577766, 578230, 578341, 578455, 578592, 578916, 580925, 581899, 587582, 604484, 605149, 605279, 605401, 605523, 605663, 605819, 606029, 607650, 614299, 644311, 647900, 656877, 660115, 689208, 689604, 689718, 689836, 689969, 690103, 690247, 690410, 690555, 696544, 696688, 698674, 698990, 714788, 714963, 715218, 715351, 717581, 719590, 720564, 720842, 721028, 723779, 723924, 724095, 724819, 726097, 726401, 726561, 726751, 730335, 733090, 766337, 770416, 776299, 776683, 776782, 776899, 777022, 777166, 777318, 777508, 779030, 803988, 804464, 804574, 804719, 804829, 804970, 805100, 805232, 805842, 806595, 809902, 811640, 820373, 821358, 821681, 831710, 835249, 835755, 835880, 836022, 836181, 836505, 838309, 846441, 864513, 865259, 865381, 865735, 885238, 886669, 910679, 915326, 915668, 916010, 916144, 916288, 916440, 918316, 941322, 943693, 947681, 969834, 1005463, 1047247, 1047628, 1048842, 1050036, 1050584, 1055115, 1080752, 1110264, 1110899, 1111226, 1111550, 1118598, 1121981, 1140407, 1141978, 1149756, 1163682, 1166209]\n"
     ]
    }
   ],
   "source": [
    "def gen_session_lick_psth(aligned_list, alignment_positions=[] window=5):\n",
    "    \n",
    "    window = window * 1000\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    for aligned_file in aligned_list:\n",
    "\n",
    "        with open(aligned_file, \"r\") as inFile:\n",
    "\n",
    "            contents = inFile.read()\n",
    "\n",
    "            timestamps = json.loads(contents)\n",
    "\n",
    "        trial_df = pd.DataFrame()\n",
    "        \n",
    "        lick_timestamps = []\n",
    "        liquid_counter = 0\n",
    "        airpuff_counter = 0\n",
    "\n",
    "        # TODO: Each segment should be its own function call ie liquidon, speakers, airpuffon\n",
    "        for (index, trial) in enumerate(timestamps[\"trialTypes\"]):\n",
    "            if trial == 1:\n",
    "                liquid_counter += 1\n",
    "                trial_name = \"Trial_\" + str(index + 1)\n",
    "                trial_type = \"Sucrose_\" + str(liquid_counter)\n",
    "                s = pd.Series(trial_type, dtype=str, name=trial_name)\n",
    "                trial_df = trial_df.append(s)\n",
    "            else:\n",
    "                airpuff_counter += 1\n",
    "                trial_name = \"Trial_\" + str(index + 1)\n",
    "                trial_type = \"Airpuff_\" + str(airpuff_counter)\n",
    "                s = pd.Series(trial_type, dtype=str, name=trial_name)\n",
    "                trial_df = trial_df.append(s)\n",
    "        \n",
    "        trial_df.index.name = \"trial\"\n",
    "        trial_df.columns = [\"trial_type\"]\n",
    "        \n",
    "        exclude_keys = [\"LickOn\", \"LickOff\", \"LiquidOff\", \"AirpuffOff\", \"trialTypes\"]\n",
    "        \n",
    "        for key in timestamps.keys():\n",
    "            if key not in exclude_keys:\n",
    "                trial_df[key] = np.nan\n",
    "        \n",
    "        speakeron_df = pd.DataFrame()\n",
    "        \n",
    "        for (index, trial) in enumerate(timestamps[\"SpeakerOn\"]):\n",
    "            trial_number = \"Trial_\" + str(index + 1)\n",
    "            speaker_on_ms = trial\n",
    "            s = pd.Series(speaker_on_ms, dtype=int, name=trial_number)\n",
    "            speakeron_df = speakeron_df.append(s)\n",
    "        \n",
    "        speakeron_df.index.name = \"trial\"\n",
    "        speakeron_df.columns = [\"SpeakerOn\"]\n",
    "        \n",
    "        trial_df.update(speakeron_df)\n",
    "        \n",
    "        speakeroff_df = pd.DataFrame()\n",
    "        \n",
    "        for (index, trial) in enumerate(timestamps[\"SpeakerOff\"]):\n",
    "            \n",
    "            trial_number = \"Trial_\" + str(index + 1)\n",
    "            speaker_off_ms = trial\n",
    "            s = pd.Series(speaker_off_ms, dtype=int, name=trial_number)\n",
    "            speakeroff_df = speakeroff_df.append(s)\n",
    "        \n",
    "        speakeroff_df.index.name = \"trial\"\n",
    "        speakeroff_df.columns = [\"SpeakerOff\"]\n",
    "        \n",
    "        trial_df.update(speakeroff_df)\n",
    "        \n",
    "        trial_df.reset_index(inplace=True)\n",
    "        trial_df.set_index(\"trial_type\", inplace=True)\n",
    "\n",
    "        \n",
    "        liquid_df = pd.DataFrame()\n",
    "\n",
    "        for (index, trial) in enumerate(timestamps[\"LiquidOn\"]):\n",
    "            \n",
    "            liquid_trial = \"Sucrose_\" + str(index + 1)\n",
    "            liquid_start_ms = trial\n",
    "            s = pd.Series(liquid_start_ms, dtype=int, name=liquid_trial)\n",
    "            liquid_df = liquid_df.append(s)\n",
    "        \n",
    "        liquid_df.index.name = \"trial_type\"\n",
    "        liquid_df.columns = [\"LiquidOn\"]\n",
    "        \n",
    "        trial_df.update(liquid_df)\n",
    "        \n",
    "\n",
    "        airpuff_df = pd.DataFrame()\n",
    "\n",
    "        for (index, trial) in enumerate(timestamps[\"AirpuffOn\"]):\n",
    "            \n",
    "            airpuff_trial = \"Airpuff_\" + str(index + 1)\n",
    "            airpuff_start_ms = trial\n",
    "            s = pd.Series(airpuff_start_ms, dtype=int, name=airpuff_trial)\n",
    "            airpuff_df = airpuff_df.append(s)\n",
    "        \n",
    "        airpuff_df.index.name = \"trial_type\"\n",
    "        \n",
    "        if airpuff_df.size > 0:\n",
    "            airpuff_df.columns = [\"AirpuffOn\"]\n",
    "            trial_df.update(airpuff_df)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        trial_df.reset_index(inplace=True)\n",
    "        trial_df.set_index(\"trial\", inplace=True)\n",
    "        \n",
    "        trial_df[\"Licks\"] = np.nan\n",
    "        \n",
    "        # Use list comprehension for lick timestamps\n",
    "        lick_timestamps = [lick for (index, lick) in enumerate(timestamps[\"LickOn\"])]\n",
    "        \n",
    "        \n",
    "        window_start_list = []\n",
    "        window_end_list = []\n",
    "        \n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        if counter == 3:\n",
    "            print(trial_df)\n",
    "            print(lick_timestamps)\n",
    "            break\n",
    "    return trial_df\n",
    "\n",
    "\n",
    "df = gen_session_lick_psth(aligned_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "d8e1e9a7-0d87-403a-81a7-9d1786ac01db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tdf = df.query('trial_type.str.contains(\"Sucrose\")', engine=\"python\") query_example\n",
    "\n",
    "#         >>> df1.set_index('Code', inplace=True)\n",
    "# >>> df1.update(df2.set_index('Code'))\n",
    "# newdf = newdf.reset_index().set_index(\"trial_type\")\n",
    "# sdf= sdf.reset_index()\n",
    "# newdf = newdf.reset_index()\n",
    "# # display(newdf, sdf)\n",
    "# a = newdf[\"LiquidOn\"].values\n",
    "# b = sdf[\"LiquidOn\"].values\n",
    "\n",
    "# # display(a)\n",
    "# # display(b)\n",
    "# newdf = newdf.replace(b, a)\n",
    "\n",
    "# # Selecting old value\n",
    "# a = df['first_set'][4]\n",
    " \n",
    "# # Selecting new value\n",
    "# b = df1['first_set'][1]\n",
    " \n",
    "# # replace values of one DataFrame with\n",
    "# # the value of another DataFrame\n",
    "# df = df.replace(a,b)\n",
    " \n",
    "# # Display the Output\n",
    "# display(df)\n",
    "# newdf[\"LiquidOn\"] = sdf[sdf[\"trial_type\"].isin(newdf[\"trial_type\"])][\"LiquidOn\"].values\n",
    "# df1['Marks'] = df2[df2['Name'].isin(df1['Name'])]['Marks'].values\n",
    "# df['second_set'] = df1.replace(df['first_set'],df['second_set'])\n",
    "# newdf[\"LiquidOn\"] = newdf.replace(newdf[\"LiquidOn\"], sdf[\"LiquidOn\"]) # works but gives index instead of values\n",
    "# pandas.concat([X[X.columns - Y.columns], Y], axis=1)\n",
    "# df1[df1['col'].str.contains('foo', regex=False)]\n",
    "# df.loc[df.query(query).index(),'ColZ']=Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3445092f-baf4-48a7-b047-caabcbefceaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
