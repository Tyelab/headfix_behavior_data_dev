{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e79f5a-afdd-4b77-a0cc-e9a072fb697f",
   "metadata": {},
   "source": [
    "# Bruker Data Grepping and Alignment Notebook\n",
    "### Jeremy Delahanty June 2021\n",
    "\n",
    "Intended to grep different files/projects/datasets from user input and retain them for use in analysis/display later. The lack of unified filenaming structures between projects will break the code... A convetion of XXX### for animal names has been established for Austin's projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236756af-545f-4c82-bfc7-7f22a45d8a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import tables\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def109c7-4a96-41a8-b0c5-a66ab01b0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_basepath = \"Y:/\"\n",
    "# project_dict = {\"specialk\": [\"learned_helplessness\", \"chronic_mild_stress\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62578e70-c52a-4a7e-9d62-b7c5d0b02268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found All Selected Teams\n",
      "Teams Returned:\n",
      "specialk \n"
     ]
    }
   ],
   "source": [
    "def grep_teams(team_selection=[], lab_basepath=\"Y:/\"):\n",
    "    \"\"\"\n",
    "    Grabs team list from server based on user's input.\n",
    "    \n",
    "    User can define which teams they want to use for their analyses and\n",
    "    the function will glob the paths for their selection.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of strings for teams of interest\n",
    "        Default is empty list\n",
    "    arg2: string\n",
    "        Basepath for server location on machine\n",
    "        Default is Y:/ for mapped Windows drive\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team path grabbed from server successfully\n",
    "    2. list\n",
    "        List of teams not found\n",
    "    \"\"\"\n",
    "\n",
    "    # Take basepath and glob all available files and directories\n",
    "    team_glob = Path(lab_basepath).glob(\"{}\".format(\"*\"))\n",
    "    \n",
    "    # Check if no team was specifically asked for, tell user we're gathering all teams\n",
    "    if team_selection == []:\n",
    "\n",
    "        print(\"Gathering all teams...\")\n",
    "\n",
    "        # List comprehension for returning all directories in Tye Lab server\n",
    "        team_list = [team for team in team_glob if team.is_dir()]\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # List comprehension for returning only directories user wants in the Tye Lab server \n",
    "        team_list = [team for team in team_glob if team.name in team_selection and team.is_dir()]\n",
    "    \n",
    "    # Create temporary list for checking if selected teams exist\n",
    "    tmp = []\n",
    "\n",
    "    # For the teams that were globbed successfully, append the team to the temp list\n",
    "    for globbed_team in team_list:\n",
    "        tmp.append(globbed_team.name)\n",
    "    \n",
    "    # Compare team selection with returned teams using sets, convert to list\n",
    "    missing_teams = list(set(team_selection) - set(tmp))\n",
    "    \n",
    "    # If the missing_teams list is empty, the program found all requested teams\n",
    "    if missing_teams == []:\n",
    "        print(\"Found All Selected Teams\")\n",
    "    \n",
    "    # Else, some teams weren't found. Tell the user which teams weren't found.\n",
    "    else:\n",
    "        print(\"Failed to find team(s):\", missing_teams)\n",
    "   \n",
    "    # Show user which teams were returned\n",
    "    print(\"Teams Returned:\")\n",
    "    for team in team_list:\n",
    "        print(\"{} \".format(team.name))\n",
    "    \n",
    "    # Return the list of projects gathered\n",
    "    return team_list, missing_teams\n",
    "\n",
    "team_list, missing_teams = grep_teams([\"specialk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d47d606e-6e66-4ffb-a644-1af2834b8e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned Directories: \n",
      "Y:\\specialk\\learned_helplessness\n"
     ]
    }
   ],
   "source": [
    "def choose_projects(team_list, project_selection={}):\n",
    "    \"\"\"\n",
    "    Generates project paths list based on user's selection.\n",
    "    \n",
    "    User can define which project they want to use for their analyses and\n",
    "    this function generates the paths for their selection.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of strings for teams of interest from grep_teams()\n",
    "    arg2: dict\n",
    "        Dictionary of values that will be used to create specific\n",
    "        paths for selected teams and their projects\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team/project Paths to grep in next steps\n",
    "    \"\"\"\n",
    "\n",
    "    # Make dictionary using the teams in team_list as keys\n",
    "    project_dict = {team: [] for team in team_list}\n",
    "\n",
    "    # For each time in the team_list, append the Path name's project's values\n",
    "    for team in team_list:\n",
    "        project_dict[team].append(project_selection[team.name])\n",
    "    \n",
    "    # Make empty project list\n",
    "    project_dir_list = []\n",
    "\n",
    "    for team in project_dict.keys():\n",
    "        for project in range(len(project_dict[team][0])):\n",
    "            project_dir_list.append(team / project_dict[team][0][project])\n",
    "    \n",
    "    print(\"Returned Directories: \")\n",
    "\n",
    "    for directory in project_dir_list:\n",
    "        print(directory)\n",
    "    \n",
    "    return project_dir_list\n",
    "    \n",
    "project_list = choose_projects(team_list, project_selection={\"specialk\": [\"learned_helplessness\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c423b7-92e8-45ba-9af8-4103534166d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing only E animals...\n",
      "learned_helplessness LHE011\n",
      "learned_helplessness LHE012\n",
      "learned_helplessness LHE013\n",
      "learned_helplessness LHE014\n",
      "learned_helplessness LHE015\n",
      "learned_helplessness LHE016\n"
     ]
    }
   ],
   "source": [
    "def choose_animals(project_list, animal_group=\"all\"):\n",
    "    \"\"\"\n",
    "    Generates animal paths list based on user's selection.\n",
    "    \n",
    "    User can define which cohort of animals they want to use \n",
    "    for their analyses. This function generates the paths for \n",
    "    their selection that meet specified conditions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of strings for projects of interest from choose_projects()\n",
    "    arg2: str\n",
    "        String of value for which animal paths to gather.\n",
    "        Default value is all.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team/project/animal Paths to grep in next steps\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create empty animal list for path generation\n",
    "    animal_list = []\n",
    "    \n",
    "    # If the animal group is left as default/specified as all, grab all animals\n",
    "    if animal_group == \"all\":\n",
    "        print(\"Grabbing all animals...\")\n",
    "        \n",
    "        # For each project directory in the project list\n",
    "        for project_dir in project_list:\n",
    "            \n",
    "            # For each animal globbed in the project directory\n",
    "            for animal in project_dir.glob(\"*\"):\n",
    "                \n",
    "                # Append the animal's path to the animal_list\n",
    "                print(project_dir.name, animal.name)\n",
    "                animal_list.append(animal)\n",
    "    \n",
    "    # Else, only select animals from the specified group\n",
    "    else:\n",
    "        print(\"Grabbing only {} animals...\".format(animal_group))\n",
    "\n",
    "        # Format the animal group with the user's input\n",
    "        animal_group = \"[A-Z]{2}\" + animal_group + \"\\d{3}\"\n",
    "        \n",
    "        # For each project_directory in project_list\n",
    "        for project_dir in project_list:\n",
    "\n",
    "            # For each animal globbed in project directory\n",
    "            for animal in project_dir.glob(\"*\"):\n",
    "                \n",
    "                # Use regex to grab only the requested animal\n",
    "                r = re.search(animal_group, string=animal.name)\n",
    "                \n",
    "                # If the search returns None, the animal didn't match the request\n",
    "                # Skip over it with pass.\n",
    "                if r is None:\n",
    "                    pass\n",
    "                \n",
    "                # If something is returned, take the match object's value and append\n",
    "                # the animal to the project directory.\n",
    "                else:\n",
    "                    print(project_dir.name, r.group(0))\n",
    "                    animal_list.append(project_dir / r.group(0))\n",
    "    \n",
    "    # Finally, return the list of animals\n",
    "    return animal_list\n",
    "\n",
    "animal_list = choose_animals(project_list, animal_group=\"E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f50cbd-7489-476a-bc55-90794f3092d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing specified directories... \n",
      " ['twop']\n",
      "\n",
      "Returning Directories:\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE012\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE013\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE014\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE015\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE016\\twop\n"
     ]
    }
   ],
   "source": [
    "def choose_data(animal_list, data_group=[], verbose=True):\n",
    "    \"\"\"\n",
    "    Generates animal's data paths list based on user's selection.\n",
    "    \n",
    "    User can define which dataset to use for the animals they \n",
    "    want to use for their analyses. This function generates the\n",
    "    paths for their selection that meet specified conditions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of paths for animals of interest from choose_animals()\n",
    "    arg2: list\n",
    "        List of strings for which datasets to gather.\n",
    "        Default value is all.\n",
    "    arg3: bool\n",
    "        Boolean argument for verbose output of paths found or\n",
    "        not found by the function. Default is True.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team/project/animal/dataset Paths to grep in\n",
    "        next steps\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create empty data list for path generation\n",
    "    data_dir_list = []\n",
    "    \n",
    "    # If data_group is left as default or specified as empty,\n",
    "    # grab all folders\n",
    "    if data_group == []:\n",
    "        print(\"Grabbing all data folders...\")\n",
    "        \n",
    "        # For each animal in the animal list\n",
    "        for animal in animal_list:\n",
    "            \n",
    "            # For the data_dir in the globbed animal_path\n",
    "            for data_dir in animal.glob(\"*\"):\n",
    "                \n",
    "                # Append the data_dir to the data_list\n",
    "                print(\"Grabbing\", animal.name, data_dir.name)\n",
    "                data_list.append(data)\n",
    "    \n",
    "    #TODO: Make verbose into its own function\n",
    "    elif len(data_group) > 0 and verbose is True:\n",
    "        print(\"Grabbing...\")\n",
    "        for data_type in data_group:\n",
    "            print(data_type, \"data\")\n",
    "\n",
    "        print(\"\\nFrom Projects(s)...\")\n",
    "        project_list = list(set([project.parent.name for project in animal_list]))\n",
    "        for project in project_list:\n",
    "            print(project)\n",
    "            \n",
    "        print(\"\\nIn Team(s)...\")\n",
    "        team_list = list(set([team.parent.parent.name for team in animal_list]))\n",
    "        for team in team_list:\n",
    "            print(team)\n",
    "        print(\"\\nFor Animals...\")\n",
    "        for animal in animal_list:\n",
    "            print(animal.name)\n",
    "        \n",
    "        print(\"\\nChecking for directories...\")\n",
    "        for animal in animal_list:\n",
    "            for data_type in data_group:\n",
    "                if (animal / data_type).is_dir():\n",
    "                    print(\"Found\", animal.name, data_type)\n",
    "                    data_dir_list.append(animal / data_type)\n",
    "                else:\n",
    "                    print(\"Not Found!\", animal.name, data_type)\n",
    "    else:\n",
    "       \n",
    "        #TODO: Write a function for checking\n",
    "        print(\"Grabbing specified directories...\", \"\\n\", data_group)\n",
    "        \n",
    "        for animal in animal_list:\n",
    "            for data_type in data_group:\n",
    "                if (animal / data_type).is_dir():\n",
    "                    data_dir_list.append(animal / data_type)\n",
    "                else:\n",
    "                    print(animal.name, data_type, \"Not Found!\")\n",
    "    \n",
    "    # Tell user which directories were returned\n",
    "    print(\"\\nReturning Directories:\")\n",
    "    for data_dir in data_dir_list:\n",
    "        print(data_dir)\n",
    "\n",
    "    return data_dir_list\n",
    "                \n",
    "\n",
    "data_dir_list = choose_data(animal_list, data_group=[\"twop\"], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac8d9c09-abd4-4bd3-ab54-11c010d7ce30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking if LHE011 twop 20210602 raw behavior is converted...\n",
      "\n",
      "WARNING!!! LHE011 twop 20210602 raw behavior not converted!\n",
      "\n",
      "Checking if LHE011 twop 20210603 raw behavior is converted...\n",
      "LHE011 twop 20210603 raw behavior converted!\n",
      "\n",
      "Checking if LHE011 twop 20210604 raw behavior is converted...\n",
      "LHE011 twop 20210604 raw behavior converted!\n",
      "\n",
      "Checking if LHE011 twop 20210605 raw behavior is converted...\n",
      "LHE011 twop 20210605 raw behavior converted!\n",
      "\n",
      "Checking if LHE011 twop 20210607 raw behavior is converted...\n",
      "LHE011 twop 20210607 raw behavior converted!\n",
      "\n",
      "Checking if LHE011 twop 20210608 raw behavior is converted...\n",
      "LHE011 twop 20210608 raw behavior converted!\n",
      "\n",
      "Checking if LHE011 twop 20210611 raw behavior is converted...\n",
      "LHE011 twop 20210611 raw behavior converted!\n",
      "\n",
      "Checking if LHE011 twop 20210612 raw behavior is converted...\n",
      "LHE011 twop 20210612 raw behavior converted!\n",
      "\n",
      "Returning checked files:\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210603\\20210603_LHE011_plane0_raw_behavior-042\\20210603_LHE011_plane0-042_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210604\\20210604_LHE011_plane0_raw_behavior-048\\20210604_LHE011_plane0-048_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210605\\20210605_LHE011_plane0_raw_behavior-056\\20210605_LHE011_plane0-056_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210607\\20210607_LHE011_plane0_raw_behavior-070\\20210607_LHE011_plane0-070_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210608\\20210608_LHE011_plane0_raw_behavior-071\\20210608_LHE011_plane0-071_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210611\\20210611_LHE011_plane0_raw_behavior-081\\20210611_LHE011_plane0-081_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210612\\20210612_LHE011_plane0_raw_behavior-084\\20210612_LHE011_plane0-084_Cycle00001_VoltageRecording_001.csv\n"
     ]
    }
   ],
   "source": [
    "def grep_twop_raw_behavior_data(data_directory, convert_missing_data=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Checks whether raw 2P behavior data has been converted to .csv for all available days.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: Path\n",
    "        Parent directory of raw behavior data file\n",
    "        \n",
    "    arg2: bool\n",
    "        Whether to invoke raw data file conversion through Bruker's raw converter\n",
    "        Default False; no containerized ripper is in place (7/29/21)...\n",
    "        \n",
    "    arg3: bool\n",
    "        Whether to print checking process for the user.\n",
    "        Default True\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        Existing raw behavior data file paths \n",
    "    \"\"\"\n",
    "    \n",
    "    checked_behavior_file_list = []\n",
    "    \n",
    "    missing_behavior_dir_list = []\n",
    "    \n",
    "    raw_behavior_dir_search = data_directory.glob(\"*/*\")\n",
    "    \n",
    "    raw_behavior_dir_dict = {result.parent : [file for file in result.glob(\"*.csv\")] for result in raw_behavior_dir_search if result.is_dir()}\n",
    "    \n",
    "    if verbose is True:\n",
    "        for key in raw_behavior_dir_dict:\n",
    "            print(\"\\nChecking if\", key.parents[1].name, key.parents[0].name, key.name, \"raw behavior is converted...\")\n",
    "            if len(raw_behavior_dir_dict[key]) == 0:\n",
    "                print(\"\\nWARNING!!!\", key.parents[1].name, key.parents[0].name, key.name, \"raw behavior not converted!\")\n",
    "                missing_behavior_dir_list.append(raw_behavior_dir_dict[key])\n",
    "            else:\n",
    "                print(key.parents[1].name, key.parents[0].name, key.name,  \"raw behavior converted!\")\n",
    "                checked_behavior_file_list.append(raw_behavior_dir_dict[key][0])\n",
    "                \n",
    "        print(\"\\nReturning checked files:\")\n",
    "        for file in checked_behavior_file_list:\n",
    "            print(file)\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        # TODO: This should be a warning/exception that is logged and/or saved somewhere maybe...\n",
    "        for key in raw_behavior_dir_dict:\n",
    "            if len(raw_behavior_dir_dict[key]) == 0:\n",
    "                print(\"WARNING!!!\", key.parents[1].name, key.parents[0].name, key.name, \"raw data not converted!\")\n",
    "            else:\n",
    "                checked_behavior_file_list.append(raw_behavior_dir_dict[key][0])\n",
    "\n",
    "    return checked_behavior_file_list\n",
    "\n",
    "checked_behavior_file_list = grep_twop_raw_behavior_data(data_dir_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc20455-f1b0-4bcf-a4da-e01cbb3bc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grep_twop_behavior_config(checked_behavior_file_list):\n",
    "    \"\"\"\n",
    "    Generates animal's configuration paths list based on user's selection\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of paths for animals of interest from choose_data()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of 2P behavior configuration Paths\n",
    "    \"\"\"\n",
    "    \n",
    "    twop_config_list = []\n",
    "    \n",
    "    for directory in checked_behavior_file_list:\n",
    "        parent_dir = directory.parents[1]\n",
    "        search = parent_dir.glob(\"*config.json\")\n",
    "        for result in search:\n",
    "            twop_config_list.append(result)\n",
    "            \n",
    "    return twop_config_list\n",
    "            \n",
    "twop_config_list = grep_twop_behavior_config(checked_behavior_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c8ee8d-0fa3-4179-9edd-94401e503196",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210603_LHE011_plane0-042_Cycle00001_VoltageRecording_001.csv\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def clean_2p_behavior(twop_raw_behavior_file, raw_keys=[\"Lick\", \"Airpuff\", \"Liquid\", \"Speaker\"], df_keys=[\"on\", \"off\"]):\n",
    "    \"\"\"\n",
    "    Takes in raw 2P behavior file produced by Bruker and generates timestamps.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: Path\n",
    "        Path to raw behavior data\n",
    "    arg2: list\n",
    "        List of raw data keys to clean\n",
    "        Default is Lick, Airpuff, Liquid, and Speaker data\n",
    "    arg3: list\n",
    "        List of keys to create for dataframe\n",
    "        Default is onset AND offset of each event\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. dict\n",
    "        Dictionary containing timestamps for each event\n",
    "    \"\"\"\n",
    "\n",
    "    # Create list of requested keys by concatenating each value in the raw and df keys arguments\n",
    "    requested_keys = []\n",
    "    \n",
    "    for raw_key in raw_keys:\n",
    "        for df_key in df_keys:\n",
    "            requested_keys.append(raw_key + \"_\" + df_key)\n",
    "    \n",
    "    \n",
    "    # Do the cleaning for the supplied raw data file\n",
    "    # Tell the user which file is being processed\n",
    "    print(\"Cleaning:\", twop_raw_behavior_file.name)\n",
    "\n",
    "    # Adapting code by Kyle Fischer, June 2021 thru line 44\n",
    "    # Read the raw .csv from Bruker's Voltage Recording\n",
    "    raw_behavior_df = pd.read_csv(twop_raw_behavior_file, index_col=\"Time(ms)\").rename(columns=lambda col:col.strip())\n",
    "\n",
    "    # Any value below 3V is certainly noise.  Set them to zero by filtering the raw dataframe\n",
    "    raw_behavior_df = raw_behavior_df > 3\n",
    "\n",
    "    # Convert all values to int for pandas.DataFrame.diff() to yield negative values for stop times\n",
    "    # and then perform the .diff() function and finally use .fillna() to eliminate NaN values\n",
    "    raw_behavior_df = raw_behavior_df.astype(int).diff().fillna(0)\n",
    "\n",
    "    # Create empty dictionary for cleaned timestamps\n",
    "    cleaned_behavior_dict = {}\n",
    "\n",
    "    # Query the raw datafile for each condition.  1 represents \"On\" signal while -1 represents \"Off\"\n",
    "    for key in requested_keys:\n",
    "        if \"on\" in key:\n",
    "            cleaned_behavior_dict[key] = raw_behavior_df.query(key.split(\"_\")[0] + \" == 1\").index.tolist()\n",
    "        else:\n",
    "            cleaned_behavior_dict[key] = raw_behavior_df.query(key.split(\"_\")[0] + \" == -1\").index.tolist()\n",
    "\n",
    "    return cleaned_behavior_dict\n",
    "\n",
    "cleaned_behavior_dict = clean_2p_behavior(checked_behavior_file_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5cc36b6-4fbe-4c74-95de-04088f310d50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_2p_trialtypes(twop_config_file):\n",
    "    \"\"\"\n",
    "    Takes in twop configuration file and gives trial types\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: Path\n",
    "        Path two behavior configuration file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        Gives list of trial types\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the json file using json package\n",
    "    with open(twop_config_file, \"r\") as inFile:\n",
    "\n",
    "        # Read the config file\n",
    "        config = inFile.read()\n",
    "\n",
    "        # Decode the file with json.loads()\n",
    "        config_contents = json.loads(config)\n",
    "\n",
    "        # Gather the trial types from the configuration\n",
    "        config_trial_types = config_contents[\"trialArray\"]\n",
    "        \n",
    "    return config_trial_types\n",
    "\n",
    "    \n",
    "config_trial_types = get_2p_trialtypes(twop_config_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1af1d39-8210-42c6-8273-7fa0ffff36f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_base_stimulus_df(config_trial_types):\n",
    "    \"\"\"\n",
    "    Creates trial DataFrame from configuration trial types\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of trial types obtained from get_2p_trialtypes()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. Pandas DataFrame\n",
    "        Returns base dataframe of trial types\n",
    "    \"\"\"\n",
    "    \n",
    "    base_behavior_df = pd.DataFrame()\n",
    "    \n",
    "    liquid_counter = 0\n",
    "    airpuff_counter = 0\n",
    "    \n",
    "    for (index, trial) in enumerate(config_trial_types):\n",
    "        if trial == 1:\n",
    "            liquid_counter += 1\n",
    "            trial_name = \"Trial_\" + str(index + 1)\n",
    "            trial_type = \"Sucrose_\" + str(liquid_counter)\n",
    "            series = pd.Series(trial_type, dtype=str, name=trial_name)\n",
    "            base_behavior_df = base_behavior_df.append(series)\n",
    "        else:\n",
    "            airpuff_counter += 1\n",
    "            trial_name = \"Trial_\" + str(index + 1)\n",
    "            trial_type = \"Airpuff_\" + str(airpuff_counter)\n",
    "            series = pd.Series(trial_type, dtype=str, name=trial_name)\n",
    "            base_behavior_df = base_behavior_df.append(series)\n",
    "\n",
    "    base_behavior_df.reset_index(inplace=True)\n",
    "    base_behavior_df.columns = [\"trial\", \"trial_type\"]\n",
    "\n",
    "    \n",
    "    return base_behavior_df\n",
    "\n",
    "base_df = build_base_stimulus_df(config_trial_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "941d2ced-b83b-4260-b590-ae006f924acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_cleaned_stimulus_df(clean_behavior_dict):\n",
    "    \"\"\"\n",
    "    Creates cleaned stimulus dataframe of timestamps from cleaned behavior dictionary\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    1. dict\n",
    "        Cleaned behavior dictionary from clean_2p_behavior()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. Pandas DataFrame\n",
    "        Dataframe of cleaned behavior timestamps per trial\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(cleaned_behavior_dict[\"Airpuff_on\"]) == 0:\n",
    "        cleaned_stimulus_df_keys = [key for key in cleaned_behavior_dict if \"Lick\" not in key and \"Airpuff\" not in key]\n",
    "    else:\n",
    "        cleaned_stimulus_df_keys = [key for key in cleaned_behavior_dict if \"Lick\" not in key]\n",
    "    \n",
    "    cleaned_stimulus_df = pd.DataFrame(columns=cleaned_stimulus_df_keys)\n",
    "    \n",
    "    for key in cleaned_stimulus_df_keys:\n",
    "        cleaned_stimulus_df[key] = pd.Series(cleaned_behavior_dict[key]).divide(1000)\n",
    "    \n",
    "    return cleaned_stimulus_df\n",
    "        \n",
    "\n",
    "clean_df = build_cleaned_stimulus_df(cleaned_behavior_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be21d865-75f8-42b9-a8db-9ef32edb378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_behavior_stimulus_df(base_df, clean_df):\n",
    "    \"\"\"\n",
    "    Joins cleaned_stimulus_df and base_behavior_df into one DataFrame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    1. Pandas DataFrame\n",
    "        Base behavior dataframe (trial types)\n",
    "    2. Pandas DataFrame\n",
    "        Cleaned stimulus dataframe (timestamps)\n",
    "    \n",
    "    Results\n",
    "    -------\n",
    "    1. Pandas DataFrame\n",
    "        Dataframe from united base and stimulus inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    merged_behavior_df = base_df.join(clean_df, how=\"outer\")\n",
    "    \n",
    "    return merged_behavior_df\n",
    "    \n",
    "merged_behavior_df = build_behavior_stimulus_df(base_df, clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffe3e4c4-bde9-430d-b412-e9fe7de11aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210603\\20210603_LHE011_plane0_merged.h5...done\n"
     ]
    }
   ],
   "source": [
    "def write_merged_behavior_hdf(checked_behavior_file, merged_behavior_df, config_trialtypes, lick_timestamps):\n",
    "    \"\"\"\n",
    "    Writes hdf5 file of cleaned data for a given session to disk\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    1. Path\n",
    "        Path of checked behavior\n",
    "    2. pandas.Dataframe\n",
    "        Cleaned behavior dataframe of trials and stimuli\n",
    "    3. list\n",
    "        List of trial types from configuration file\n",
    "    4. list\n",
    "        List of lick timestamps from cleaned_behavior_dict\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    filename_pattern = re.compile(\"\\d{8}_[A-Z]{3}\\d{3}_plane\\d{1}\")\n",
    "    \n",
    "    re_filename_result = re.search(pattern=filename_pattern, string=checked_behavior_file.name)\n",
    "    \n",
    "    hdf_filename = checked_behavior_file.parents[1] / (re_filename_result.group(0) + \"_merged.h5\")\n",
    "    \n",
    "    merged_behavior_df.set_index(\"trial\", inplace=True)\n",
    "    \n",
    "    trial_types = pd.Series(config_trialtypes)\n",
    "    \n",
    "    lick_timestamps = pd.Series(lick_timestamps)\n",
    "    \n",
    "    hdf_store = pd.HDFStore(hdf_filename)\n",
    "    \n",
    "    hdf_store.append(value=merged_behavior_df, key=\"stimulus_timestamps\")\n",
    "    \n",
    "    hdf_store.append(value=trial_types, key=\"trial_types\")\n",
    "    \n",
    "    hdf_store.append(value=lick_timestamps, key=\"lick_timestamps\")\n",
    "    \n",
    "    tables.file._open_files.close_all()\n",
    "    \n",
    "\n",
    "write_merged_behavior_hdf(checked_behavior_file_list[0], merged_behavior_df, config_trial_types, cleaned_behavior_dict[\"Lick_on\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39096bcd-a331-440d-be44-31d05ae1187e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking if LHE011 twop 20210602 raw behavior is converted...\n",
      "\n",
      "WARNING!!! LHE011 twop 20210602 raw behavior not converted!\n",
      "\n",
      "Checking if LHE011 twop 20210603 raw behavior is converted...\n",
      "LHE011 twop 20210603 raw behavior converted!\n",
      "\n",
      "Checking if LHE011 twop 20210604 raw behavior is converted...\n",
      "LHE011 twop 20210604 raw behavior converted!\n",
      "\n",
      "Checking if LHE011 twop 20210605 raw behavior is converted...\n",
      "LHE011 twop 20210605 raw behavior converted!\n",
      "\n",
      "Checking if LHE011 twop 20210607 raw behavior is converted...\n",
      "LHE011 twop 20210607 raw behavior converted!\n",
      "\n",
      "Checking if LHE011 twop 20210608 raw behavior is converted...\n",
      "LHE011 twop 20210608 raw behavior converted!\n",
      "\n",
      "Checking if LHE011 twop 20210611 raw behavior is converted...\n",
      "LHE011 twop 20210611 raw behavior converted!\n",
      "\n",
      "Checking if LHE011 twop 20210612 raw behavior is converted...\n",
      "LHE011 twop 20210612 raw behavior converted!\n",
      "\n",
      "Returning checked files:\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210603\\20210603_LHE011_plane0_raw_behavior-042\\20210603_LHE011_plane0-042_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210604\\20210604_LHE011_plane0_raw_behavior-048\\20210604_LHE011_plane0-048_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210605\\20210605_LHE011_plane0_raw_behavior-056\\20210605_LHE011_plane0-056_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210607\\20210607_LHE011_plane0_raw_behavior-070\\20210607_LHE011_plane0-070_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210608\\20210608_LHE011_plane0_raw_behavior-071\\20210608_LHE011_plane0-071_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210611\\20210611_LHE011_plane0_raw_behavior-081\\20210611_LHE011_plane0-081_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210612\\20210612_LHE011_plane0_raw_behavior-084\\20210612_LHE011_plane0-084_Cycle00001_VoltageRecording_001.csv\n",
      "Cleaning: 20210603_LHE011_plane0-042_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210603\\20210603_LHE011_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210604_LHE011_plane0-048_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210604\\20210604_LHE011_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210605_LHE011_plane0-056_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210605\\20210605_LHE011_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210607_LHE011_plane0-070_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210607\\20210607_LHE011_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210608_LHE011_plane0-071_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210608\\20210608_LHE011_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210611_LHE011_plane0-081_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210611\\20210611_LHE011_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210612_LHE011_plane0-084_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210612\\20210612_LHE011_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking if LHE012 twop 20210602 raw behavior is converted...\n",
      "\n",
      "WARNING!!! LHE012 twop 20210602 raw behavior not converted!\n",
      "\n",
      "Checking if LHE012 twop 20210603 raw behavior is converted...\n",
      "LHE012 twop 20210603 raw behavior converted!\n",
      "\n",
      "Checking if LHE012 twop 20210604 raw behavior is converted...\n",
      "LHE012 twop 20210604 raw behavior converted!\n",
      "\n",
      "Checking if LHE012 twop 20210605 raw behavior is converted...\n",
      "LHE012 twop 20210605 raw behavior converted!\n",
      "\n",
      "Checking if LHE012 twop 20210606 raw behavior is converted...\n",
      "LHE012 twop 20210606 raw behavior converted!\n",
      "\n",
      "Checking if LHE012 twop 20210607 raw behavior is converted...\n",
      "LHE012 twop 20210607 raw behavior converted!\n",
      "\n",
      "Checking if LHE012 twop 20210610 raw behavior is converted...\n",
      "LHE012 twop 20210610 raw behavior converted!\n",
      "\n",
      "Checking if LHE012 twop 20210611 raw behavior is converted...\n",
      "LHE012 twop 20210611 raw behavior converted!\n",
      "\n",
      "Returning checked files:\n",
      "Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210603\\20210603_LHE012_plane0_raw_behavior-043\\20210603_LHE012_plane0-043_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210604\\20210604_LHE012_plane0_raw_behavior-049\\20210604_LHE012_plane0-049_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210605\\20210605_LHE012_plane0_raw_behavior-057\\20210605_LHE012_plane0-057_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210606\\20210606_LHE012_plane0_raw_behavior-063\\20210606_LHE012_plane0-063_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210607\\20210607_LHE012_plane0_raw_behavior-069\\20210607_LHE012_plane0-069_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210610\\20210610_LHE012_plane0_raw_behavior-078\\20210610_LHE012_plane0-078_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210611\\20210611_LHE012_plane0_raw_behavior-082\\20210611_LHE012_plane0-082_Cycle00001_VoltageRecording_001.csv\n",
      "Cleaning: 20210603_LHE012_plane0-043_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210603\\20210603_LHE012_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210604_LHE012_plane0-049_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210604\\20210604_LHE012_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210605_LHE012_plane0-057_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210605\\20210605_LHE012_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210606_LHE012_plane0-063_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210606\\20210606_LHE012_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210607_LHE012_plane0-069_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210607\\20210607_LHE012_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210610_LHE012_plane0-078_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210610\\20210610_LHE012_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210611_LHE012_plane0-082_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210611\\20210611_LHE012_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking if LHE013 twop 20210602 raw behavior is converted...\n",
      "\n",
      "WARNING!!! LHE013 twop 20210602 raw behavior not converted!\n",
      "\n",
      "Checking if LHE013 twop 20210603 raw behavior is converted...\n",
      "LHE013 twop 20210603 raw behavior converted!\n",
      "\n",
      "Checking if LHE013 twop 20210604 raw behavior is converted...\n",
      "LHE013 twop 20210604 raw behavior converted!\n",
      "\n",
      "Checking if LHE013 twop 20210605 raw behavior is converted...\n",
      "LHE013 twop 20210605 raw behavior converted!\n",
      "\n",
      "Checking if LHE013 twop 20210606 raw behavior is converted...\n",
      "LHE013 twop 20210606 raw behavior converted!\n",
      "\n",
      "Checking if LHE013 twop 20210609 raw behavior is converted...\n",
      "\n",
      "WARNING!!! LHE013 twop 20210609 raw behavior not converted!\n",
      "\n",
      "Checking if LHE013 twop 20210610 raw behavior is converted...\n",
      "LHE013 twop 20210610 raw behavior converted!\n",
      "\n",
      "Returning checked files:\n",
      "Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210603\\20210603_LHE013_plane0_raw_behavior-044\\20210603_LHE013_plane0-044_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210604\\20210604_LHE013_plane0_raw_behavior-051\\20210604_LHE013_plane0-051_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210605\\20210605_LHE013_plane0_raw_behavior-058\\20210605_LHE013_plane0-058_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210606\\20210606_LHE013_plane0_raw_behavior-064\\20210606_LHE013_plane0-064_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210610\\20210610_LHE013_plane0_raw_behavior-076\\20210610_LHE013_plane0-076_Cycle00001_VoltageRecording_001.csv\n",
      "Cleaning: 20210603_LHE013_plane0-044_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210603\\20210603_LHE013_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210604_LHE013_plane0-051_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210604\\20210604_LHE013_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210605_LHE013_plane0-058_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210605\\20210605_LHE013_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210606_LHE013_plane0-064_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210606\\20210606_LHE013_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210610_LHE013_plane0-076_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210610\\20210610_LHE013_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking if LHE014 twop 20210602 raw behavior is converted...\n",
      "\n",
      "WARNING!!! LHE014 twop 20210602 raw behavior not converted!\n",
      "\n",
      "Checking if LHE014 twop 20210603 raw behavior is converted...\n",
      "LHE014 twop 20210603 raw behavior converted!\n",
      "\n",
      "Checking if LHE014 twop 20210604 raw behavior is converted...\n",
      "LHE014 twop 20210604 raw behavior converted!\n",
      "\n",
      "Checking if LHE014 twop 20210605 raw behavior is converted...\n",
      "LHE014 twop 20210605 raw behavior converted!\n",
      "\n",
      "Checking if LHE014 twop 20210606 raw behavior is converted...\n",
      "LHE014 twop 20210606 raw behavior converted!\n",
      "\n",
      "Checking if LHE014 twop 20210609 raw behavior is converted...\n",
      "LHE014 twop 20210609 raw behavior converted!\n",
      "\n",
      "Checking if LHE014 twop 20210610 raw behavior is converted...\n",
      "LHE014 twop 20210610 raw behavior converted!\n",
      "\n",
      "Returning checked files:\n",
      "Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210603\\20210603_LHE014_plane0_raw_behavior-045\\20210603_LHE014_plane0-045_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210604\\20210604_LHE014_plane0_raw_behavior-052\\20210604_LHE014_plane0-052_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210605\\20210605_LHE014_plane0_raw_behavior-059\\20210605_LHE014_plane0-059_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210606\\20210606_LHE014_plane0_raw_behavior-065\\20210606_LHE014_plane0-065_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210609\\20210609_LHE014_plane0_raw_behavior-073\\20210609_LHE014_plane0-073_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210610\\20210610_LHE014_plane0_raw_behavior-077\\20210610_LHE014_plane0-077_Cycle00001_VoltageRecording_001.csv\n",
      "Cleaning: 20210603_LHE014_plane0-045_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210603\\20210603_LHE014_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210604_LHE014_plane0-052_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210604\\20210604_LHE014_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210605_LHE014_plane0-059_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210605\\20210605_LHE014_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210606_LHE014_plane0-065_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210606\\20210606_LHE014_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210609_LHE014_plane0-073_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210609\\20210609_LHE014_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210610_LHE014_plane0-077_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210610\\20210610_LHE014_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking if LHE015 twop 20210602 raw behavior is converted...\n",
      "\n",
      "WARNING!!! LHE015 twop 20210602 raw behavior not converted!\n",
      "\n",
      "Checking if LHE015 twop 20210603 raw behavior is converted...\n",
      "LHE015 twop 20210603 raw behavior converted!\n",
      "\n",
      "Checking if LHE015 twop 20210604 raw behavior is converted...\n",
      "LHE015 twop 20210604 raw behavior converted!\n",
      "\n",
      "Checking if LHE015 twop 20210605 raw behavior is converted...\n",
      "LHE015 twop 20210605 raw behavior converted!\n",
      "\n",
      "Checking if LHE015 twop 20210606 raw behavior is converted...\n",
      "LHE015 twop 20210606 raw behavior converted!\n",
      "\n",
      "Checking if LHE015 twop 20210609 raw behavior is converted...\n",
      "LHE015 twop 20210609 raw behavior converted!\n",
      "\n",
      "Checking if LHE015 twop 20210610 raw behavior is converted...\n",
      "LHE015 twop 20210610 raw behavior converted!\n",
      "\n",
      "Checking if LHE015 twop 20210611 raw behavior is converted...\n",
      "LHE015 twop 20210611 raw behavior converted!\n",
      "\n",
      "Returning checked files:\n",
      "Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210603\\20210603_LHE015_plane0_raw_behavior-046\\20210603_LHE015_plane0-046_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210604\\20210604_LHE015_plane0_raw_behavior-053\\20210604_LHE015_plane0-053_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210605\\20210605_LHE015_plane0_raw_behavior-060\\20210605_LHE015_plane0-060_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210606\\20210606_LHE015_plane0_raw_behavior-066\\20210606_LHE015_plane0-066_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210609\\20210609_LHE015_plane0_raw_behavior-074\\20210609_LHE015_plane0-074_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210610\\20210610_LHE015_plane0_raw_behavior-079\\20210610_LHE015_plane0-079_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210611\\20210611_LHE015_plane0_raw_behavior_10psi-083\\20210611_LHE015_plane0_10psi-083_Cycle00001_VoltageRecording_001.csv\n",
      "Cleaning: 20210603_LHE015_plane0-046_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210603\\20210603_LHE015_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210604_LHE015_plane0-053_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210604\\20210604_LHE015_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210605_LHE015_plane0-060_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210605\\20210605_LHE015_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210606_LHE015_plane0-066_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210606\\20210606_LHE015_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210609_LHE015_plane0-074_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210609\\20210609_LHE015_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210610_LHE015_plane0-079_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210610\\20210610_LHE015_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking if LHE016 twop 20210602 raw behavior is converted...\n",
      "\n",
      "WARNING!!! LHE016 twop 20210602 raw behavior not converted!\n",
      "\n",
      "Checking if LHE016 twop 20210603 raw behavior is converted...\n",
      "LHE016 twop 20210603 raw behavior converted!\n",
      "\n",
      "Checking if LHE016 twop 20210604 raw behavior is converted...\n",
      "LHE016 twop 20210604 raw behavior converted!\n",
      "\n",
      "Checking if LHE016 twop 20210605 raw behavior is converted...\n",
      "LHE016 twop 20210605 raw behavior converted!\n",
      "\n",
      "Checking if LHE016 twop 20210606 raw behavior is converted...\n",
      "LHE016 twop 20210606 raw behavior converted!\n",
      "\n",
      "Checking if LHE016 twop 20210609 raw behavior is converted...\n",
      "LHE016 twop 20210609 raw behavior converted!\n",
      "\n",
      "Checking if LHE016 twop 20210610 raw behavior is converted...\n",
      "LHE016 twop 20210610 raw behavior converted!\n",
      "\n",
      "Returning checked files:\n",
      "Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210603\\20210603_LHE016_plane0_raw_behavior-047\\20210603_LHE016_plane0-047_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210604\\20210604_LHE016_plane0_raw_behavior-055\\20210604_LHE016_plane0-055_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210605\\20210605_LHE016_plane0_raw_behavior-061\\20210605_LHE016_plane0-061_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210606\\20210606_LHE016_plane0_raw_behavior-067\\20210606_LHE016_plane0-067_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210609\\20210609_LHE016_plane0_raw_behavior-075\\20210609_LHE016_plane0-075_Cycle00001_VoltageRecording_001.csv\n",
      "Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210610\\20210610_LHE016_plane0_raw_behavior-080\\20210610_LHE016_plane0-080_Cycle00001_VoltageRecording_001.csv\n",
      "Cleaning: 20210603_LHE016_plane0-047_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210603\\20210603_LHE016_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210604_LHE016_plane0-055_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210604\\20210604_LHE016_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210605_LHE016_plane0-061_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210605\\20210605_LHE016_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210606_LHE016_plane0-067_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210606\\20210606_LHE016_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210609_LHE016_plane0-075_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210609\\20210609_LHE016_plane0_merged.h5...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: 20210610_LHE016_plane0-080_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210610\\20210610_LHE016_plane0_merged.h5...done\n"
     ]
    }
   ],
   "source": [
    "def merge_bruker_twop_data(data_dir_list):\n",
    "    \n",
    "    for data_dir in data_dir_list:\n",
    "        checked_behavior_file_list = grep_twop_raw_behavior_data(data_dir)\n",
    "        twop_config_list = grep_twop_behavior_config(checked_behavior_file_list)\n",
    "        for file, config in zip(checked_behavior_file_list, twop_config_list):\n",
    "            cleaned_behavior_dict = clean_2p_behavior(file)\n",
    "            config_trial_types = get_2p_trialtypes(config)\n",
    "            base_df = build_base_stimulus_df(config_trial_types)\n",
    "            clean_df = build_cleaned_stimulus_df(cleaned_behavior_dict)\n",
    "            merged_behavior_df = build_behavior_stimulus_df(base_df, clean_df)\n",
    "            write_merged_behavior_hdf(file, merged_behavior_df, config_trial_types, cleaned_behavior_dict[\"Lick_on\"])\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "merge_bruker_twop_data(data_dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0151462-83b9-4509-ac28-38bd9521beb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     392132\n",
       "1     421719\n",
       "2     421890\n",
       "3     422130\n",
       "4     422309\n",
       "5     422491\n",
       "6     423184\n",
       "7     591335\n",
       "8     591479\n",
       "9     591567\n",
       "10    591677\n",
       "11    605736\n",
       "12    605900\n",
       "13    610886\n",
       "14    615826\n",
       "15    617953\n",
       "16    618105\n",
       "17    618269\n",
       "18    618645\n",
       "19    618873\n",
       "20    619079\n",
       "21    619889\n",
       "22    622463\n",
       "23    622623\n",
       "24    623106\n",
       "25    624959\n",
       "26    627530\n",
       "27    627693\n",
       "28    627891\n",
       "29    689528\n",
       "30    690924\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_hdf(\"Y:/specialk/learned_helplessness/LHE011/twop/20210603/20210603_LHE011_plane0_merged.h5\", key=\"lick_timestamps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e5e9690-3517-4b65-9483-f73e3a34ff0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     392132\n",
      "1     421719\n",
      "2     421890\n",
      "3     422130\n",
      "4     422309\n",
      "5     422491\n",
      "6     423184\n",
      "7     591335\n",
      "8     591479\n",
      "9     591567\n",
      "10    591677\n",
      "11    605736\n",
      "12    605900\n",
      "13    610886\n",
      "14    615826\n",
      "15    617953\n",
      "16    618105\n",
      "17    618269\n",
      "18    618645\n",
      "19    618873\n",
      "20    619079\n",
      "21    619889\n",
      "22    622463\n",
      "23    622623\n",
      "24    623106\n",
      "25    624959\n",
      "26    627530\n",
      "27    627693\n",
      "28    627891\n",
      "29    689528\n",
      "30    690924\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def gen_session_lick_raster(merged_file, pre_window=5, post_window=5):\n",
    "        \n",
    "    # use converters with eval function to retain list from csv format\n",
    "    session_df = pd.read_hdf(merged_file, key=\"stimulus_timestamps\", index_col=\"trial\")\n",
    "    \n",
    "    lick_series = pd.read_hdf(merged_file, key=\"lick_timestamps\")\n",
    "    \n",
    "    session_df[\"centered_licks\"] = np.empty((len(session_df), 0)).tolist()\n",
    "    \n",
    "\n",
    "#     for lick_list in session_df[\"LiquidOn_licks\"].items():\n",
    "#         for lick in lick_list[1]:\n",
    "#             value = (lick - session_df[\"LiquidOn\"].loc[lick_list[0]]).round(3)\n",
    "#             session_df[\"centered_licks\"].loc[lick_list[0]].append(value)\n",
    "\n",
    "#     session_liquid_centered_lists = [lick for lick in session_df[\"centered_licks\"]]\n",
    "\n",
    "#     session_lick_raster_fig, ax = plt.subplots()\n",
    "#     plt.xlim(-5, 5)\n",
    "#     plt.title(aligned_file.stem + \" Entire Session Lick Raster\")\n",
    "#     plt.xlabel(\"Time (s)\")\n",
    "#     plt.ylabel(\"Trial Number\")\n",
    "#     plt.axvline(x=0, ymin=0, ymax=40, color=\"green\", label=\"test\")\n",
    "#     ax.eventplot(positions=session_liquid_centered_lists)\n",
    "\n",
    "gen_session_lick_raster(\"Y:/specialk/learned_helplessness/LHE011/twop/20210603/20210603_LHE011_plane0_merged.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a492228b-0b4c-4292-83a9-98b1b43b5845",
   "metadata": {},
   "source": [
    "# BELOW THIS POINT IS DEPRECATED/ONLY FOR PICKING APART SCRAPS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "44b8c815-42bb-4ef4-9b90-75c69c8613c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Needs Merging: LHE011 20210603\n",
      "Session Needs Merging: LHE011 20210604\n",
      "Session Needs Merging: LHE011 20210605\n",
      "Session Needs Merging: LHE011 20210607\n",
      "Session Needs Merging: LHE011 20210608\n",
      "Session Needs Merging: LHE011 20210611\n",
      "Session Needs Merging: LHE011 20210612\n",
      "Merging: 20210603_LHE011_plane0-042_Cycle00001_VoltageRecording_001.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-698-34ce0ef6818a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmerged_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m \u001b[0mmerged_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_2p_behavior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwop_raw_beh_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtwop_config_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-698-34ce0ef6818a>\u001b[0m in \u001b[0;36mmerge_2p_behavior\u001b[1;34m(cleaned_behavior_dict, config_trial_types)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;31m# Append trial types to merged file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"trialTypes\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;31m# Create the new file using json package\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: This needs to be done by writing things to HDF5/Zarr in NWB format so a complete dataset for a given session is available for analysis\n",
    "def merge_2p_behavior(cleaned_behavior_dict, config_trial_types):\n",
    "    \"\"\"\n",
    "    Merges one twop behavior, configuration, video, and microscopy files' data together.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: dict\n",
    "        Dictionary of cleaned behavior data timestamps\n",
    "    arg2: list\n",
    "        List of trialtypes from configuration file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of merged 2P behavior paths\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Use Kyle's code to align these files with the relative timestamps of microscopy\n",
    "    # TODO: Get zipping of these two lists to work so finding config file is already completed...\n",
    "    # TODO: Make force overwrite/recompile the different datasets\n",
    "    # TODO: Make verbose version of this function\n",
    "    needs_merging = []\n",
    "    \n",
    "    merged_list = []\n",
    "    \n",
    "    # Give name_date pattern for the files we're aligning\n",
    "    name_date_pattern = re.compile(\"\\d{8}_[A-Z]{3}\\d{3}\")\n",
    "    \n",
    "    # First, check for cleaned data\n",
    "    for directory in twop_raw_beh_list:\n",
    "        \n",
    "        # Glob for the aligned json file\n",
    "        cleaned_check = directory.parents[1].glob(\"*_merged.json\")\n",
    "        \n",
    "        # Make a list using the result of the glob using list comprehension\n",
    "        clean_checklist = [session for session in cleaned_check]\n",
    "        \n",
    "        # If the list is empty, the session needs merging\n",
    "        if len(clean_checklist) == 0:\n",
    "            \n",
    "            # Show which session needs alignment and append the directory to needs_merging list\n",
    "            print(\"Session Needs Merging:\", directory.parents[3].name, directory.parents[1].name)\n",
    "            needs_merging.append(directory)\n",
    "        \n",
    "        # Else, the session has already been merged.  Append the filepath to the merged_list.\n",
    "        else:\n",
    "            merged_file = clean_checklist[0]\n",
    "            print(\"Session already merged:\", merged_file)\n",
    "            merged_list.append(merged_file)\n",
    "    \n",
    "    # Now, do the merging\n",
    "    for raw_file in needs_merging:\n",
    "        print(\"Merging:\", raw_file.name)\n",
    "        \n",
    "        merged = {}\n",
    "        \n",
    "        parent_folder = raw_file.parents[1]\n",
    "        \n",
    "        # From Kyle Fischer, June 2021 thru line 92\n",
    "        raw_behavior_df = pd.read_csv(raw_file, index_col=\"Time(ms)\").rename(columns=lambda col:col.strip())\n",
    "        \n",
    "        # Any value below 3V is not signal, turn it to zero by filtering\n",
    "        # values so all that remains are values greater than 3. All else\n",
    "        # will be 0.\n",
    "        raw_behavior_df = raw_behavior_df > 3\n",
    "        \n",
    "        # Convert all values to int; is necessary for pd.df.diff() to produce\n",
    "        # negative values used for stop times of each event\n",
    "        raw_behavior_df = raw_behavior_df.astype(int)\n",
    "        \n",
    "        # Take the diff of each column; gives start and stop of each signal\n",
    "        raw_behavior_df = raw_behavior_df.diff()\n",
    "\n",
    "        # Replace any NaN values with 0\n",
    "        raw_behavior_df = raw_behavior_df.fillna(0)\n",
    "        \n",
    "        # Grab start and stop values for licks\n",
    "        merged[\"LickOn\"] = raw_behavior_df[raw_behavior_df[\"Lick\"] == 1].index.tolist()\n",
    "        merged[\"LickOff\"] = raw_behavior_df[raw_behavior_df[\"Lick\"] == -1].index.tolist()\n",
    "        \n",
    "        # Grab start and stop values for Airpuff Solenoid\n",
    "        merged[\"AirpuffOn\"] = raw_behavior_df[raw_behavior_df[\"Airpuff\"] == 1].index.tolist()\n",
    "        merged[\"AirpuffOff\"] = raw_behavior_df[raw_behavior_df[\"Airpuff\"] == -1].index.tolist()\n",
    "        \n",
    "        # Grab start and stop values for Liquid Solenoid\n",
    "        merged[\"LiquidOn\"] = raw_behavior_df[raw_behavior_df[\"Liquid\"] == 1].index.tolist()\n",
    "        merged[\"LiquidOff\"] = raw_behavior_df[raw_behavior_df[\"Liquid\"] == -1].index.tolist()\n",
    "        \n",
    "        # Grab start and stop values for Speaker\n",
    "        merged[\"SpeakerOn\"] = raw_behavior_df[raw_behavior_df[\"Speaker\"] == 1].index.tolist()\n",
    "        merged[\"SpeakerOff\"] = raw_behavior_df[raw_behavior_df[\"Speaker\"] == -1].index.tolist()\n",
    "        \n",
    "        \n",
    "        #TODO: This should be just one regex, not sure why complete isn't working...\n",
    "        \n",
    "        # Perform the regex for the name and date of the file\n",
    "        r_name_date = re.search(pattern=name_date_pattern, string=raw_file.name)\n",
    "        \n",
    "        # Give the pattern for the plane of interest\n",
    "        plane_pattern = \"_plane\\d{1}\"\n",
    "        \n",
    "        # Perform the regex for the plane number of the file\n",
    "        r_plane = re.search(pattern=plane_pattern, string=raw_file.name)\n",
    "        \n",
    "        # Concatenate strings into final merged file as json type\n",
    "        merged_name = r_name_date.group(0) + r_plane.group(0) + \"_merged.json\"\n",
    "        \n",
    "        # Append the parent folder with this name to create the file later\n",
    "        merged_filename = parent_folder / merged_name\n",
    "        \n",
    "        # Grab the config file for this plane from the parent folder\n",
    "        config_glob = parent_folder.glob(\"*.json\")\n",
    "        \n",
    "        # Config gather the config file result in a list\n",
    "        config_file_result = [config for config in config_glob]\n",
    "        \n",
    "        # The config file is the only element of this list, not sure how to retain only the relevant file without lists...\n",
    "        config_file = config_file_result[0]\n",
    "        \n",
    "        # Open the json file using json package\n",
    "        with open(config_file, \"r\") as inFile:\n",
    "            \n",
    "            # The configuration is the read file\n",
    "            config = inFile.read()\n",
    "            \n",
    "            # Load the contents of the config with json.loads()\n",
    "            config_contents = json.loads(config)\n",
    "            \n",
    "            # Gather the trial types from the configuration\n",
    "            trial_types = config_contents[\"trialArray\"]\n",
    "        \n",
    "        # Append trial types to merged file\n",
    "        merged_df[\"trialTypes\"] = trial_types\n",
    "\n",
    "        # Create the new file using json package\n",
    "        with open(merged_filename, \"w\") as outFile:\n",
    "            \n",
    "            # Use json.dump to write aligned_dictionary to file\n",
    "            json.dump(merged, outFile)\n",
    "        \n",
    "        # Tell user the file has been written\n",
    "        print(\"Written\", merged_filename)\n",
    "        aligned_list.append(merged_filename)\n",
    "        \n",
    "    return merged_list\n",
    "\n",
    "merged_list = merge_2p_behavior(twop_raw_beh_list, twop_config_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "5722d9a1-ffdc-4492-a88a-b68ac60228f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Needs Alignment: LHE011 20210603\n",
      "Session Needs Alignment: LHE011 20210604\n",
      "Session Needs Alignment: LHE011 20210605\n",
      "Session Needs Alignment: LHE011 20210607\n",
      "Session Needs Alignment: LHE011 20210608\n",
      "Session Needs Alignment: LHE011 20210611\n",
      "Session Needs Alignment: LHE011 20210612\n",
      "Session Needs Alignment: LHE012 20210603\n",
      "Session Needs Alignment: LHE012 20210604\n",
      "Session Needs Alignment: LHE012 20210605\n",
      "Session Needs Alignment: LHE012 20210606\n",
      "Session Needs Alignment: LHE012 20210607\n",
      "Session Needs Alignment: LHE012 20210610\n",
      "Session Needs Alignment: LHE012 20210611\n",
      "Session Needs Alignment: LHE013 20210603\n",
      "Session Needs Alignment: LHE013 20210604\n",
      "Session Needs Alignment: LHE013 20210605\n",
      "Session Needs Alignment: LHE013 20210606\n",
      "Session Needs Alignment: LHE013 20210610\n",
      "Session Needs Alignment: LHE014 20210603\n",
      "Session Needs Alignment: LHE014 20210604\n",
      "Session Needs Alignment: LHE014 20210605\n",
      "Session Needs Alignment: LHE014 20210606\n",
      "Session Needs Alignment: LHE014 20210609\n",
      "Session Needs Alignment: LHE014 20210610\n",
      "Session Needs Alignment: LHE015 20210603\n",
      "Session Needs Alignment: LHE015 20210604\n",
      "Session Needs Alignment: LHE015 20210605\n",
      "Session Needs Alignment: LHE015 20210606\n",
      "Session Needs Alignment: LHE015 20210609\n",
      "Session Needs Alignment: LHE015 20210610\n",
      "Session Needs Alignment: LHE015 20210611\n",
      "Session Needs Alignment: LHE016 20210603\n",
      "Session Needs Alignment: LHE016 20210604\n",
      "Session Needs Alignment: LHE016 20210605\n",
      "Session Needs Alignment: LHE016 20210606\n",
      "Session Needs Alignment: LHE016 20210609\n",
      "Session Needs Alignment: LHE016 20210610\n"
     ]
    }
   ],
   "source": [
    "# TODO: This alignment function should be used to align lick rasters specifically, these alignment positions should not be written out to disk\n",
    "# However, I will be keeping it as a means of hosting static datasets for use on a sideproject that will display static datasets online for the lab...\n",
    "\n",
    "\n",
    "# TODO: Write the merged datafile using some of this code, eventually should be incorporated into raster plot generation/psth/lick probability functions\n",
    "# TODO: Get this to incorporate the training data from the headfix boxes\n",
    "# TODO: Should not rely upon aligned file\n",
    "def align_2p_behavior(merged_list, alignment_positions=[\"LiquidOn\", \"SpeakerOn\", \"AirpuffOn\"], prewindow=5, postwindow=5):\n",
    "    \"\"\"\n",
    "    Aligns twop behavior to different positions from user within specific windows\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of paths for merged behavior data\n",
    "    arg2: list\n",
    "        Alignment positions to store\n",
    "    arg3: int\n",
    "        Window (in seconds) for pre/post event alignment\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of aligned 2P behavior paths\n",
    "    \"\"\"\n",
    "\n",
    "    needs_alignment = []\n",
    "    \n",
    "    aligned_list = []\n",
    "    \n",
    "    # First, check for cleaned data\n",
    "    for directory in merged_list:\n",
    "        \n",
    "        clean_name_pattern = \"*aligned_window\" + str(window) + \"*\"\n",
    "        \n",
    "        # Glob for the aligned json file\n",
    "        cleaned_check = directory.parents[0].glob(clean_name_pattern)\n",
    "        \n",
    "        # Make a list using the result of the glob using list comprehension\n",
    "        clean_checklist = [session for session in cleaned_check]\n",
    "        \n",
    "        # If the list is empty, the session needs alignment\n",
    "        if len(clean_checklist) == 0:\n",
    "            \n",
    "            # Show which session needs alignment and append the directory to needs_merging list\n",
    "            print(\"Session Needs Alignment:\", directory.parents[2].name, directory.parents[0].name)\n",
    "            needs_alignment.append(directory)\n",
    "        \n",
    "        # Else, the session has already been aligned.  Append the filepath to the aligned_list.\n",
    "        else:\n",
    "            aligned_file = clean_checklist[0]\n",
    "            print(\"Session already aligned:\", aligned_file)\n",
    "            aligned_list.append(aligned_file)\n",
    "\n",
    "    for merged_file in needs_alignment:\n",
    "\n",
    "        with open(merged_file, \"r\") as inFile:\n",
    "\n",
    "            contents = inFile.read()\n",
    "\n",
    "            timestamps = json.loads(contents)\n",
    "\n",
    "        trial_df = pd.DataFrame()\n",
    "        \n",
    "        lick_timestamps = []\n",
    "        liquid_counter = 0\n",
    "        airpuff_counter = 0\n",
    "\n",
    "        # TODO: Each segment should be its own function call ie liquidon, speakers, airpuffon\n",
    "        for (index, trial) in enumerate(timestamps[\"trialTypes\"]):\n",
    "            if trial == 1:\n",
    "                liquid_counter += 1\n",
    "                trial_name = \"Trial_\" + str(index + 1)\n",
    "                trial_type = \"Sucrose_\" + str(liquid_counter)\n",
    "                s = pd.Series(trial_type, dtype=str, name=trial_name)\n",
    "                trial_df = trial_df.append(s)\n",
    "            else:\n",
    "                airpuff_counter += 1\n",
    "                trial_name = \"Trial_\" + str(index + 1)\n",
    "                trial_type = \"Airpuff_\" + str(airpuff_counter)\n",
    "                s = pd.Series(trial_type, dtype=str, name=trial_name)\n",
    "                trial_df = trial_df.append(s)\n",
    "        \n",
    "        trial_df.index.name = \"trial\"\n",
    "        trial_df.columns = [\"trial_type\"]\n",
    "        \n",
    "        exclude_keys = [\"LickOn\", \"LickOff\", \"LiquidOff\", \"AirpuffOff\", \"trialTypes\"]\n",
    "        \n",
    "        for key in timestamps.keys():\n",
    "            if key not in exclude_keys:\n",
    "                trial_df[key] = np.nan\n",
    "        \n",
    "        speakeron_df = pd.DataFrame()\n",
    "        \n",
    "        for (index, trial) in enumerate(timestamps[\"SpeakerOn\"]):\n",
    "            trial_number = \"Trial_\" + str(index + 1)\n",
    "            speaker_on_ms = trial\n",
    "            s = pd.Series(speaker_on_ms, name=trial_number)\n",
    "            speakeron_df = speakeron_df.append(s)\n",
    "        \n",
    "        speakeron_df.index.name = \"trial\"\n",
    "        speakeron_df.columns = [\"SpeakerOn\"]\n",
    "        \n",
    "        trial_df.update(speakeron_df)\n",
    "        \n",
    "        trial_df[\"SpeakerOn\"] = trial_df[\"SpeakerOn\"].divide(1000)\n",
    "        \n",
    "        speakeroff_df = pd.DataFrame()\n",
    "        \n",
    "        for (index, trial) in enumerate(timestamps[\"SpeakerOff\"]):\n",
    "            \n",
    "            trial_number = \"Trial_\" + str(index + 1)\n",
    "            speaker_off_ms = trial\n",
    "            s = pd.Series(speaker_off_ms, dtype=int, name=trial_number)\n",
    "            speakeroff_df = speakeroff_df.append(s)\n",
    "        \n",
    "        speakeroff_df.index.name = \"trial\"\n",
    "        speakeroff_df.columns = [\"SpeakerOff\"]\n",
    "        \n",
    "        trial_df.update(speakeroff_df)\n",
    "        \n",
    "        trial_df[\"SpeakerOff\"] = trial_df[\"SpeakerOff\"].divide(1000)\n",
    "        \n",
    "        trial_df.reset_index(inplace=True)\n",
    "        trial_df.set_index(\"trial_type\", inplace=True)\n",
    "\n",
    "        \n",
    "        liquid_df = pd.DataFrame()\n",
    "\n",
    "        for (index, trial) in enumerate(timestamps[\"LiquidOn\"]):\n",
    "            \n",
    "            liquid_trial = \"Sucrose_\" + str(index + 1)\n",
    "            liquid_start_ms = trial\n",
    "            s = pd.Series(liquid_start_ms, dtype=int, name=liquid_trial)\n",
    "            liquid_df = liquid_df.append(s)\n",
    "        \n",
    "        liquid_df.index.name = \"trial_type\"\n",
    "        liquid_df.columns = [\"LiquidOn\"]\n",
    "        \n",
    "        trial_df.update(liquid_df)\n",
    "        \n",
    "        trial_df[\"LiquidOn\"] = trial_df[\"LiquidOn\"].divide(1000)\n",
    "        \n",
    "        airpuff_df = pd.DataFrame()\n",
    "\n",
    "        for (index, trial) in enumerate(timestamps[\"AirpuffOn\"]):\n",
    "            airpuff_trial = \"Airpuff_\" + str(index + 1)\n",
    "            airpuff_start_ms = trial\n",
    "            s = pd.Series(airpuff_start_ms, dtype=int, name=airpuff_trial)\n",
    "            airpuff_df = airpuff_df.append(s)\n",
    "        \n",
    "        airpuff_df.index.name = \"trial_type\"\n",
    "        \n",
    "        if airpuff_df.size > 0:\n",
    "            airpuff_df.columns = [\"AirpuffOn\"]\n",
    "            trial_df.update(airpuff_df)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        trial_df.reset_index(inplace=True)\n",
    "        trial_df.set_index(\"trial\", inplace=True)\n",
    "        \n",
    "        trial_df[\"AirpuffOn\"] = trial_df[\"AirpuffOn\"].divide(1000)\n",
    "        \n",
    "        # Use list comprehension for lick timestamps\n",
    "        lick_timestamps = [lick for (index, lick) in enumerate(timestamps[\"LickOn\"])]\n",
    "        \n",
    "        lick_timestamps = [lick/1000 for lick in lick_timestamps]\n",
    "        \n",
    "        # Create columns for aligning licks to different events and then create lick lists\n",
    "        # This is terrible and a better way to assign licks to a list should be found, but\n",
    "        # without using a relational system I'm not sure how I can alter this...\n",
    "        # TODO: Make this a function that is iterated over instead of multiple for loops like this...\n",
    "        # Possible solutions: Create ITI starts/stops as dataframe values and evaluate against range\n",
    "        \n",
    "        for target in alignment_positions:\n",
    "            column = target + \"_licks\"\n",
    "            trial_df[column] = np.empty((len(trial_df), 0)).tolist()\n",
    "            for lick_time in lick_timestamps:\n",
    "                for value in trial_df[target].items():\n",
    "                    if (value[1] - prewindow) <= lick_time <= (value[1] + postwindow):\n",
    "                        tmp = value[0]\n",
    "                        trial_df[column].loc[tmp].append(lick_time)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        # Create empty lists for ITI licks\n",
    "        trial_df[\"ITI_licks\"] = np.empty((len(trial_df), 0)).tolist()\n",
    "\n",
    "        # Get leftover ITI licks by creating sets out of aligned licks\n",
    "        accounted_licks = []\n",
    "        for value in trial_df[\"LiquidOn_licks\"].items():\n",
    "            if len(value[1]) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                for licks in value[1]:\n",
    "                    accounted_licks.append(licks)\n",
    "        \n",
    "        accounted_licks = set(accounted_licks)\n",
    "        licks_set = set(lick_timestamps) \n",
    "        iti_licks = list(licks_set - accounted_licks)\n",
    "\n",
    "\n",
    "        # Again, a bad way to do this. Too many loops and likely to be quite slow...\n",
    "        for lick_time in iti_licks:\n",
    "            for value in trial_df[\"ITI_licks\"].items():\n",
    "                tmp = value[0]\n",
    "                if (trial_df[\"SpeakerOn\"].loc[tmp] - prewindow) <= lick_time <= (trial_df[\"LiquidOn\"].loc[tmp] + postwindow):\n",
    "                    trial_df[\"ITI_licks\"].loc[tmp].append(lick_time)\n",
    "\n",
    "        aligned_filename = merged_file.stem\n",
    "        aligned_filename = aligned_filename.replace(\"merged\", \"aligned\")\n",
    "        aligned_filename = aligned_filename + \"_\" + \"window\" + str(window) + \".csv\"\n",
    "        \n",
    "        aligned_file_path = merged_file.parents[0] / aligned_filename\n",
    "        \n",
    "        aligned_list.append(aligned_file_path)\n",
    "        \n",
    "        trial_df.to_csv(aligned_file_path)\n",
    "\n",
    "    return aligned_list\n",
    "\n",
    "\n",
    "aligned_list = align_2p_behavior(merged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d7e62f-d559-46a9-a377-abe5a9a426ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
