{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e79f5a-afdd-4b77-a0cc-e9a072fb697f",
   "metadata": {},
   "source": [
    "# Data Grepping Notebook\n",
    "### Jeremy Delahanty June 2021\n",
    "\n",
    "Intended to grep different files/projects/datasets from user input and retain them for use in analysis/display later. The lack of unified filenaming structures between projects will break the code... A convetion of XXX### for animal names, or something similar, should be adopted for all animals in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "236756af-545f-4c82-bfc7-7f22a45d8a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def109c7-4a96-41a8-b0c5-a66ab01b0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_basepath = \"Y:/\"\n",
    "# project_dict = {\"specialk\": [\"learned_helplessness\", \"chronic_mild_stress\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62578e70-c52a-4a7e-9d62-b7c5d0b02268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found All Selected Teams\n",
      "Teams Returned:\n",
      "specialk \n"
     ]
    }
   ],
   "source": [
    "def grep_teams(team_selection=[], lab_basepath=\"Y:/\"):\n",
    "    \"\"\"\n",
    "    Grabs team list from server based on user's input.\n",
    "    \n",
    "    User can define which teams they want to use for their analyses and\n",
    "    the function will glob the paths for their selection.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of strings for teams of interest\n",
    "        Default is empty list\n",
    "    arg2: string\n",
    "        Basepath for server location on machine\n",
    "        Default is Y:/ for mapped Windows drive\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team path grabbed from server successfully\n",
    "    2. list\n",
    "        List of teams not found\n",
    "    \"\"\"\n",
    "\n",
    "    # Take basepath and glob all available files and directories\n",
    "    team_glob = Path(lab_basepath).glob(\"{}\".format(\"*\"))\n",
    "    \n",
    "    # The selected teams are what the user provides as team_selection\n",
    "    selected_teams = team_selection\n",
    "    \n",
    "    # Check if no team was specifically asked for, tell user we're gathering all teams\n",
    "    if selected_teams == []:\n",
    "\n",
    "        print(\"Gathering all teams...\")\n",
    "\n",
    "        # List comprehension for returning all directories in Tye Lab server\n",
    "        team_list = [team for team in team_glob if team.is_dir()]\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # List comprehension for returning only directories user wants in the Tye Lab server \n",
    "        team_list = [team for team in team_glob if team.name in selected_teams and team.is_dir()]\n",
    "    \n",
    "    # Create temporary list for checking if selected teams exist\n",
    "    tmp = []\n",
    "\n",
    "    # For the teams that were globbed successfully, append the team to the temp list\n",
    "    for globbed_team in team_list:\n",
    "        tmp.append(globbed_team.name)\n",
    "    \n",
    "    # Compare team selection with returned teams using sets, convert to list\n",
    "    missing_teams = list(set(selected_teams) - set(tmp))\n",
    "    \n",
    "    # If the missing_teams list is empty, the program found all requested teams\n",
    "    if missing_teams == []:\n",
    "        print(\"Found All Selected Teams\")\n",
    "    \n",
    "    # Else, some teams weren't found. Tell the user which teams weren't found.\n",
    "    else:\n",
    "        print(\"Failed to find team(s):\", missing_teams)\n",
    "   \n",
    "    # Show user which teams were returned\n",
    "    print(\"Teams Returned:\")\n",
    "    for team in team_list:\n",
    "        print(\"{} \".format(team.name))\n",
    "    \n",
    "    # Return the list of projects gathered\n",
    "    return team_list, missing_teams\n",
    "\n",
    "team_list, missing_teams = grep_teams([\"specialk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d47d606e-6e66-4ffb-a644-1af2834b8e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned Directories: \n",
      "Y:\\specialk\\learned_helplessness\n",
      "Y:\\specialk\\chronic_mild_stress\n"
     ]
    }
   ],
   "source": [
    "def choose_projects(team_list, project_selection={}):\n",
    "    \"\"\"\n",
    "    Generates project paths list based on user's selection.\n",
    "    \n",
    "    User can define which project they want to use for their analyses and\n",
    "    this function generates the paths for their selection.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of strings for teams of interest from grep_teams()\n",
    "    arg2: dict\n",
    "        Dictionary of values that will be used to create specific\n",
    "        paths for selected teams and their projects\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team/project Paths to grep in next steps\n",
    "    \"\"\"\n",
    "\n",
    "    # Make dictionary using the teams in team_list as keys\n",
    "    project_dict = {team: [] for team in team_list}\n",
    "\n",
    "    # For each time in the team_list, append the Path name's project's values\n",
    "    for team in team_list:\n",
    "        project_dict[team].append(project_selection[team.name])\n",
    "    \n",
    "    # Make empty project list\n",
    "    project_dir_list = []\n",
    "\n",
    "    for team in project_dict.keys():\n",
    "        for project in range(len(project_dict[team][0])):\n",
    "            project_dir_list.append(team / project_dict[team][0][project])\n",
    "    \n",
    "    print(\"Returned Directories: \")\n",
    "\n",
    "    for directory in project_dir_list:\n",
    "        print(directory)\n",
    "    \n",
    "    return project_dir_list\n",
    "    \n",
    "project_list = choose_projects(team_list, project_selection={\"specialk\": [\"learned_helplessness\", \"chronic_mild_stress\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c423b7-92e8-45ba-9af8-4103534166d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing only E animals...\n",
      "learned_helplessness LHE011\n",
      "learned_helplessness LHE012\n",
      "learned_helplessness LHE013\n",
      "learned_helplessness LHE014\n",
      "learned_helplessness LHE015\n",
      "learned_helplessness LHE016\n",
      "chronic_mild_stress CSE001\n"
     ]
    }
   ],
   "source": [
    "def choose_animals(project_list, animal_group=\"all\"):\n",
    "    \"\"\"\n",
    "    Generates animal paths list based on user's selection.\n",
    "    \n",
    "    User can define which cohort of animals they want to use \n",
    "    for their analyses. This function generates the paths for \n",
    "    their selection that meet specified conditions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of strings for projects of interest from choose_projects()\n",
    "    arg2: str\n",
    "        String of value for which animal paths to gather.\n",
    "        Default value is all.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team/project/animal Paths to grep in next steps\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create empty animal list for path generation\n",
    "    animal_list = []\n",
    "    \n",
    "    # If the animal group is left as default/specified as all, grab all animals\n",
    "    if animal_group == \"all\":\n",
    "        print(\"Grabbing all animals...\")\n",
    "        \n",
    "        # For each project directory in the project list\n",
    "        for project_dir in project_list:\n",
    "            \n",
    "            # For each animal globbed in the project directory\n",
    "            for animal in project_dir.glob(\"*\"):\n",
    "                \n",
    "                # Append the animal's path to the animal_list\n",
    "                print(project_dir.name, animal.name)\n",
    "                animal_list.append(animal)\n",
    "    \n",
    "    # Else, only select animals from the specified group\n",
    "    else:\n",
    "        print(\"Grabbing only {} animals...\".format(animal_group))\n",
    "\n",
    "        # Format the animal group with the user's input\n",
    "        animal_group = \"[A-Z]{2}\" + animal_group + \"\\d{3}\"\n",
    "        \n",
    "        # For each project_directory in project_list\n",
    "        for project_dir in project_list:\n",
    "\n",
    "            # For each animal globbed in project directory\n",
    "            for animal in project_dir.glob(\"*\"):\n",
    "                \n",
    "                # Use regex to grab only the requested animal\n",
    "                r = re.search(animal_group, string=animal.name)\n",
    "                \n",
    "                # If the search returns None, the animal didn't match the request\n",
    "                # Skip over it with pass.\n",
    "                if r is None:\n",
    "                    pass\n",
    "                \n",
    "                # If something is returned, take the match object's value and append\n",
    "                # the animal to the project directory.\n",
    "                else:\n",
    "                    print(project_dir.name, r.group(0))\n",
    "                    animal_list.append(project_dir / r.group(0))\n",
    "    \n",
    "    # Finally, return the list of animals\n",
    "    return animal_list\n",
    "\n",
    "animal_list = choose_animals(project_list, animal_group=\"E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7f50cbd-7489-476a-bc55-90794f3092d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing specified directories...\n",
      "\n",
      "CSE001 twop Not Found!\n",
      "\n",
      "Returning Directories:\n",
      "Y:\\specialk\\learned_helplessness\\LHE011\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE012\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE013\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE014\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE015\\twop\n",
      "Y:\\specialk\\learned_helplessness\\LHE016\\twop\n"
     ]
    }
   ],
   "source": [
    "def choose_data(animal_list, data_group=[], verbose=True):\n",
    "    \"\"\"\n",
    "    Generates animal's data paths list based on user's selection.\n",
    "    \n",
    "    User can define which dataset to use for the animals they \n",
    "    want to use for their analyses. This function generates the\n",
    "    paths for their selection that meet specified conditions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1: list\n",
    "        List of paths for animals of interest from choose_animals()\n",
    "    arg2: list\n",
    "        List of strings for which datasets to gather.\n",
    "        Default value is all.\n",
    "    arg3: bool\n",
    "        Boolean argument for verbose output of paths found or\n",
    "        not found by the function. Default is True.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    1. list\n",
    "        List of team/project/animal/dataset Paths to grep in\n",
    "        next steps\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create empty data list for path generation\n",
    "    data_list = []\n",
    "    \n",
    "    # If data_group is left as default or specified as empty,\n",
    "    # grab all folders\n",
    "    if data_group == []:\n",
    "        print(\"Grabbing all data folders...\")\n",
    "        \n",
    "        # For each animal in the animal list\n",
    "        for animal in animal_list:\n",
    "            \n",
    "            # For the data_dir in the globbed animal_path\n",
    "            for data_dir in animal.glob(\"*\"):\n",
    "                \n",
    "                # Append the data_dir to the data_list\n",
    "                print(\"Grabbing\", animal.name, data_dir.name)\n",
    "                data_list.append(data)\n",
    "    \n",
    "    #TODO: Make verbose into its own function\n",
    "    elif len(data_group) > 0 and verbose is True:\n",
    "        print(\"Grabbing...\")\n",
    "        for data_type in data_group:\n",
    "            print(data_type, \"data\")\n",
    "\n",
    "        print(\"\\nFrom Projects(s)...\")\n",
    "        project_list = list(set([project.parent.name for project in animal_list]))\n",
    "        for project in project_list:\n",
    "            print(project)\n",
    "            \n",
    "        print(\"\\nIn Team(s)...\")\n",
    "        team_list = list(set([team.parent.parent.name for team in animal_list]))\n",
    "        for team in team_list:\n",
    "            print(team)\n",
    "        print(\"\\nFor Animals...\")\n",
    "        for animal in animal_list:\n",
    "            print(animal.name)\n",
    "        \n",
    "        print(\"\\nChecking for directories...\")\n",
    "        for animal in animal_list:\n",
    "            for data_type in data_group:\n",
    "                if (animal / data_type).is_dir():\n",
    "                    print(\"Found\", animal.name, data_type)\n",
    "                    data_list.append(animal / data_type)\n",
    "                else:\n",
    "                    print(\"Not Found!\", animal.name, data_type)\n",
    "    else:\n",
    "       \n",
    "        #TODO: Write a function for checking\n",
    "        print(\"Grabbing specified directories...\\n\")\n",
    "        \n",
    "        for animal in animal_list:\n",
    "            for data_type in data_group:\n",
    "                if (animal / data_type).is_dir():\n",
    "                    data_list.append(animal / data_type)\n",
    "                else:\n",
    "                    print(animal.name, data_type, \"Not Found!\")\n",
    "    \n",
    "    # Tell user which directories were returned\n",
    "    print(\"\\nReturning Directories:\")\n",
    "    for data_dir in data_list:\n",
    "        print(data_dir)\n",
    "\n",
    "    return data_list\n",
    "                \n",
    "\n",
    "data_list = choose_data(animal_list, data_group=[\"twop\"], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9fb7173-b10f-43f4-a69d-0d9917e53076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grep_twop_behavior_raw(data_list, session_type=[]):\n",
    "\n",
    "    twop_raw_beh_list = []\n",
    "    \n",
    "    \n",
    "    for directory in data_list:\n",
    "        all_search = directory.glob(\"*/*raw_behavior*/*.csv\")\n",
    "        for result in all_search:\n",
    "            twop_raw_beh_list.append(result)\n",
    "            \n",
    "    return twop_raw_beh_list\n",
    "\n",
    "twop_raw_beh_list = grep_twop_behavior_raw(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfc20455-f1b0-4bcf-a4da-e01cbb3bc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grep_twop_behavior_config(twop_raw_beh_list):\n",
    "    \n",
    "    twop_config_list = []\n",
    "    \n",
    "    #TODO: Use .parents instead of parent.parent\n",
    "    for directory in twop_raw_beh_list:\n",
    "        search = directory.parent.parent.glob(\"*.json\")\n",
    "        for result in search:\n",
    "            twop_config_list.append(result)\n",
    "            \n",
    "    return twop_config_list\n",
    "            \n",
    "twop_config_list = grep_twop_behavior_config(twop_raw_beh_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "44b8c815-42bb-4ef4-9b90-75c69c8613c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210603\\20210603_LHE011_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210604\\20210604_LHE011_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210605\\20210605_LHE011_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210607\\20210607_LHE011_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210608\\20210608_LHE011_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210611\\20210611_LHE011_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE011\\twop\\20210612\\20210612_LHE011_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210603\\20210603_LHE012_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210604\\20210604_LHE012_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210605\\20210605_LHE012_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210606\\20210606_LHE012_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210607\\20210607_LHE012_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210610\\20210610_LHE012_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE012\\twop\\20210611\\20210611_LHE012_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210603\\20210603_LHE013_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210604\\20210604_LHE013_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210605\\20210605_LHE013_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210606\\20210606_LHE013_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE013\\twop\\20210610\\20210610_LHE013_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210603\\20210603_LHE014_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210604\\20210604_LHE014_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210605\\20210605_LHE014_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210606\\20210606_LHE014_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210609\\20210609_LHE014_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE014\\twop\\20210610\\20210610_LHE014_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210603\\20210603_LHE015_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210604\\20210604_LHE015_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210605\\20210605_LHE015_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210606\\20210606_LHE015_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210609\\20210609_LHE015_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210610\\20210610_LHE015_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE015\\twop\\20210611\\20210611_LHE015_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210603\\20210603_LHE016_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210604\\20210604_LHE016_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210605\\20210605_LHE016_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210606\\20210606_LHE016_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210609\\20210609_LHE016_plane0_aligned.json\n",
      "Session already aligned: Y:\\specialk\\learned_helplessness\\LHE016\\twop\\20210610\\20210610_LHE016_plane0_aligned.json\n"
     ]
    }
   ],
   "source": [
    "def align_2p_behavior(twop_raw_beh_list, twop_config_list, twop_microscopy_list=[]):\n",
    "    \n",
    "    # TODO: Use Kyle's code to align these files with the relative timestamps of microscopy\n",
    "    # TODO: Get zipping of these two lists to work so finding config file is already completed...\n",
    "    # TODO: Make force overwrite/recompile the different datasets\n",
    "    # TODO: Make verbose version of this function\n",
    "    needs_alignment = []\n",
    "    \n",
    "    aligned_list = []\n",
    "    \n",
    "    # First, check for cleaned data\n",
    "    for directory in twop_raw_beh_list:\n",
    "        \n",
    "        # Glob for the aligned json file\n",
    "        cleaned_check = directory.parents[1].glob(\"*_aligned.json\")\n",
    "        \n",
    "        # Make a list using the result of the glob using list comprehension\n",
    "        clean_checklist = [session for session in cleaned_check]\n",
    "        \n",
    "        # If the list is empty, the session needs alignment\n",
    "        if len(clean_checklist) == 0:\n",
    "            \n",
    "            # Show which session needs alignment and append the directory to needs_alignment list\n",
    "            print(\"Session Needs Alignment:\", directory.parents[3].name, directory.parents[1].name)\n",
    "            needs_alignment.append(directory)\n",
    "        \n",
    "        # Else, the session has already been aligned.  Append the filepath to the aligned_list.\n",
    "        else:\n",
    "            aligned_file = clean_checklist[0]\n",
    "            print(\"Session already aligned:\", aligned_file)\n",
    "            aligned_list.append(aligned_file)\n",
    "    \n",
    "    # Now, do the cleaning\n",
    "    \n",
    "    for raw_file in needs_alignment:\n",
    "        print(\"Aligning:\", raw_file.name)\n",
    "        \n",
    "        aligned = {}\n",
    "        \n",
    "        parent_folder = raw_file.parents[1]\n",
    "        \n",
    "        raw_behavior_df = pd.read_csv(raw_file, index_col=\"Time(ms)\").rename(columns=lambda col:col.strip())\n",
    "        \n",
    "        # Any value below 3V is not signal, turn it to zero by filtering\n",
    "        # values so all that remains are values greater than 3. All else\n",
    "        # will be 0.\n",
    "        raw_behavior_df = raw_behavior_df > 3\n",
    "        \n",
    "        # Convert all values to int; is necessary for pd.df.diff() to produce\n",
    "        # negative values used for stop times of each event\n",
    "        raw_behavior_df = raw_behavior_df.astype(int)\n",
    "        \n",
    "        # Take the diff of each column; gives start and stop of each signal\n",
    "        raw_behavior_df = raw_behavior_df.diff()\n",
    "\n",
    "        # Replace any NaN values with 0\n",
    "        raw_behavior_df = raw_behavior_df.fillna(0)\n",
    "        \n",
    "        # Grab start and stop values for licks\n",
    "        aligned[\"LickOn\"] = raw_behavior_df[raw_behavior_df[\"Lick\"] == 1].index.tolist()\n",
    "        aligned[\"LickOff\"] = raw_behavior_df[raw_behavior_df[\"Lick\"] == -1].index.tolist()\n",
    "        \n",
    "        # Grab start and stop values for Airpuff Solenoid\n",
    "        aligned[\"AirpuffOn\"] = raw_behavior_df[raw_behavior_df[\"Airpuff\"] == 1].index.tolist()\n",
    "        aligned[\"AirpuffOff\"] = raw_behavior_df[raw_behavior_df[\"Airpuff\"] == -1].index.tolist()\n",
    "        \n",
    "        # Grab start and stop values for Liquid Solenoid\n",
    "        aligned[\"LiquidOn\"] = raw_behavior_df[raw_behavior_df[\"Liquid\"] == 1].index.tolist()\n",
    "        aligned[\"LiquidOff\"] = raw_behavior_df[raw_behavior_df[\"Liquid\"] == -1].index.tolist()\n",
    "        \n",
    "        # Grab start and stop values for Speaker\n",
    "        aligned[\"SpeakerOn\"] = raw_behavior_df[raw_behavior_df[\"Speaker\"] == 1].index.tolist()\n",
    "        aligned[\"SpeakerOff\"] = raw_behavior_df[raw_behavior_df[\"Speaker\"] == -1].index.tolist()\n",
    "        \n",
    "        #TODO: This should be just one regex, not sure why complete isn't working...\n",
    "        # Give name_date pattern for the files we're aligning\n",
    "        name_date_pattern = \"\\d{8}_[A-Z]{3}\\d{3}\"\n",
    "        \n",
    "        # Perform the regex for the name and date of the file\n",
    "        r_name_date = re.search(pattern=name_date_pattern, string=raw_file.name)\n",
    "        \n",
    "        # Give the pattern for the plane of interest\n",
    "        plane_pattern = \"_plane\\d{1}\"\n",
    "        \n",
    "        # Perform the regex for the plane number of the file\n",
    "        r_plane = re.search(pattern=plane_pattern, string=raw_file.name)\n",
    "        \n",
    "        # Concatenate strings into final aligned file as json type\n",
    "        aligned_name = r_name_date.group(0) + r_plane.group(0) + \"_aligned.json\"\n",
    "        \n",
    "        # Append the parent folder with this name to create the file later\n",
    "        aligned_filename = parent_folder / aligned_name\n",
    "        \n",
    "        # Grab the config file for this plane from the parent folder\n",
    "        config_glob = parent_folder.glob(\"*.json\")\n",
    "        \n",
    "        # Config gather the config file result in a list\n",
    "        config_file_result = [config for config in config_glob]\n",
    "        \n",
    "        # The config file is the only element of this list, not sure how to retain only the relevant file without lists...\n",
    "        config_file = config_file_result[0]\n",
    "        \n",
    "        # Open the json file using json package \n",
    "        with open(config_file, \"r\") as inFile:\n",
    "            \n",
    "            # The configuration is the read file\n",
    "            config = inFile.read()\n",
    "            \n",
    "            # Load the contents of the config with json.loads()\n",
    "            config_contents = json.loads(config)\n",
    "            \n",
    "            # Gather the trial types from the configuration\n",
    "            trial_types = config_contents[\"trialArray\"]\n",
    "        \n",
    "        # Append trial types to aligned_file\n",
    "        aligned[\"trialTypes\"] = trial_types\n",
    "\n",
    "        # Create the new file using json package\n",
    "        with open(aligned_filename, \"w\") as outFile:\n",
    "            \n",
    "            # Use json.dump to write aligned_dictionary to file\n",
    "            json.dump(aligned, outFile)\n",
    "        \n",
    "        # Tell user the file has been written\n",
    "        print(\"Written\", aligned_filename)\n",
    "        aligned_list.append(aligned_filename)\n",
    "        \n",
    "    return aligned_list\n",
    "\n",
    "aligned_list = align_2p_behavior(twop_raw_beh_list, twop_config_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "984ea731-e43f-4dff-a33d-8ada2b587ef8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'trialTypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Miniconda3\\envs\\headfix_behavior_data\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'trialTypes'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-265-3f5dc160e450>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'trialTypes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtemp_trial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtemp_trial\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'trialTypes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimestamps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'trialTypes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtimestamps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'trialTypes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtemp_trial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAirpuffOn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimestamps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AirpuffOn'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mairpuff_count\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\headfix_behavior_data\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\headfix_behavior_data\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'trialTypes'"
     ]
    }
   ],
   "source": [
    "allTrials = pd.DataFrame()\n",
    "\n",
    "airpuff_count = 0\n",
    "liquid_count = 0\n",
    "\n",
    "for i in range(len(timestamps['trialTypes'])):\n",
    "    temp_trial = pd.DataFrame()\n",
    "    temp_trial['trialTypes'][i] = timestamps['trialTypes'][i]\n",
    "    if timestamps['trialTypes'][i] == 0:\n",
    "        temp_trial.AirpuffOn = timestamps['AirpuffOn'][airpuff_count]\n",
    "        airpuff_count = airpuff_count + 1\n",
    "    if timestamps['trialTypes'][i] == 1:\n",
    "        temp_trial.LiquidOn = timestamps['LiquidOn'][airpuff_count]\n",
    "        liquid_count = liquid_count + 1\n",
    "    temp_trial.SpeakerOn = timestamps['SpeakerOn'][i]\n",
    "#     temp_trial.AirpuffOn = timestamps['AirpuffOn'][i]\n",
    "    allTrials = pd.concat([allTrials, temp_trial])\n",
    "allTrials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "5722d9a1-ffdc-4492-a88a-b68ac60228f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "def gen_session_lick_psth(aligned_list, window=10):\n",
    "    \n",
    "    window = window * 1000\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    for aligned_file in aligned_list:\n",
    "\n",
    "        with open(aligned_file, \"r\") as inFile:\n",
    "\n",
    "            contents = inFile.read()\n",
    "\n",
    "            timestamps = json.loads(contents)\n",
    "\n",
    "        trial_df = pd.DataFrame()\n",
    "\n",
    "        liquid_counter = 0\n",
    "        airpuff_counter = 0\n",
    "\n",
    "        for (index, trial) in enumerate(timestamps[\"trialTypes\"]):\n",
    "            if trial == 1:\n",
    "                liquid_counter += 1\n",
    "                trial_name = \"Trial_\" + str(index + 1)\n",
    "                trial_type = \"Sucrose_\" + str(liquid_counter)\n",
    "                s = pd.Series(trial_type, dtype=str, name=trial_name)\n",
    "                trial_df = trial_df.append(s)\n",
    "            else:\n",
    "                airpuff_counter += 1\n",
    "                trial_name = \"Trial_\" + str(index + 1)\n",
    "                trial_type = \"Airpuff_\" + str(airpuff_counter)\n",
    "                s = pd.Series(trial_type, dtype=str, name=trial_name)\n",
    "                trial_df = trial_df.append(s)\n",
    "        trial_df.index.name = \"trialtype_trial\"\n",
    "        counter += 1\n",
    "        if counter == 1:\n",
    "            break\n",
    "\n",
    "\n",
    "gen_session_lick_psth(aligned_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e1e9a7-0d87-403a-81a7-9d1786ac01db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
